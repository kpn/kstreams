{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kstreams kstreams is a library/micro framework to use with kafka . It has simple kafka streams implementation that gives certain guarantees, see below. Requirements python 3.8+ Installation pip install kstreams You will need a worker, we recommend aiorun pip install aiorun Usage import aiorun from kstreams import create_engine , ConsumerRecord stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ( \"local--kstream\" ) async def consume ( cr : ConsumerRecord ): print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . value } \" ) async def produce (): payload = b '{\"message\": \"Hello world!\"}' for i in range ( 5 ): metadata = await stream_engine . send ( \"local--kstreams\" , value = payload ) print ( f \"Message sent: { metadata } \" ) async def start (): await stream_engine . start () await produce () async def shutdown ( loop ): await stream_engine . stop () if __name__ == \"__main__\" : aiorun . run ( start (), stop_on_unhandled_errors = True , shutdown_callback = shutdown ) Kafka configuration Configure kafka using the kafka backend provided. Development This repo requires the use of poetry instead of pip. Note : If you want to have the virtualenv in the same path as the project first you should run poetry config --local virtualenvs.in-project true To install the dependencies just execute: poetry install Then you can activate the virtualenv with poetry shell Run test: ./scripts/test Run code linting ( black and isort ) ./scripts/lint Commit messages The use of commitizen is recommended. Commitizen is part of the dev dependencies. cz commit","title":"Introduction"},{"location":"#kstreams","text":"kstreams is a library/micro framework to use with kafka . It has simple kafka streams implementation that gives certain guarantees, see below.","title":"Kstreams"},{"location":"#requirements","text":"python 3.8+","title":"Requirements"},{"location":"#installation","text":"pip install kstreams You will need a worker, we recommend aiorun pip install aiorun","title":"Installation"},{"location":"#usage","text":"import aiorun from kstreams import create_engine , ConsumerRecord stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ( \"local--kstream\" ) async def consume ( cr : ConsumerRecord ): print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . value } \" ) async def produce (): payload = b '{\"message\": \"Hello world!\"}' for i in range ( 5 ): metadata = await stream_engine . send ( \"local--kstreams\" , value = payload ) print ( f \"Message sent: { metadata } \" ) async def start (): await stream_engine . start () await produce () async def shutdown ( loop ): await stream_engine . stop () if __name__ == \"__main__\" : aiorun . run ( start (), stop_on_unhandled_errors = True , shutdown_callback = shutdown )","title":"Usage"},{"location":"#kafka-configuration","text":"Configure kafka using the kafka backend provided.","title":"Kafka configuration"},{"location":"#development","text":"This repo requires the use of poetry instead of pip. Note : If you want to have the virtualenv in the same path as the project first you should run poetry config --local virtualenvs.in-project true To install the dependencies just execute: poetry install Then you can activate the virtualenv with poetry shell Run test: ./scripts/test Run code linting ( black and isort ) ./scripts/lint","title":"Development"},{"location":"#commit-messages","text":"The use of commitizen is recommended. Commitizen is part of the dev dependencies. cz commit","title":"Commit messages"},{"location":"backends/","text":"Backends The main idea of a backend is to supply the necessary configuration to create a connection with the backend. kstreams currently has support for Kafka as a backend. kstreams.backends.kafka.Kafka The Kafka backend validates the given attributes. It uses pydantic internally. Attributes: Name Type Description bootstrap_servers List [ str ] kafka list of hostname:port security_protocol SecurityProtocol Protocol used to communicate with brokers ssl_context Optional [ SSLContext ] a python std ssl.SSLContext instance, you can generate it with create_ssl_context or create_ssl_context_from_mem sasl_mechanism SaslMechanism Authentication mechanism when security_protocol is configured for SASL_PLAINTEXT or SASL_SSL sasl_plain_username Optional [ str ] username for sasl PLAIN authentication sasl_plain_password Optional [ str ] password for sasl PLAIN authentication sasl_oauth_token_provider Optional [ str ] smth Raises: Type Description ValidationError a pydantic.ValidationError exception PLAINTEXT Example from kstreams.backends.kafka import Kafka from kstreams import create_engine , Stream backend = Kafka ( bootstrap_servers = [ \"localhost:9092\" ]) stream_engine = create_engine ( title = \"my-stream-engine\" , backend = backend ) SSL Example Create SSL context import ssl from kstreams.backends.kafka import Kafka from kstreams import create_engine , utils , Stream def get_ssl_context () -> ssl . SSLContext : return utils . create_ssl_context ( cafile = \"certificate-authority-file-path\" , capath = \"points-to-directory-with-several-ca-certificates\" , cadata = \"same-as-cafile-but-ASCII-or-bytes-format\" , certfile = \"client-certificate-file-name\" , keyfile = \"client-private-key-file-name\" , password = \"password-to-load-certificate-chain\" , ) backend = Kafka ( bootstrap_servers = [ \"localhost:9094\" ], security_protocol = \"SSL\" , ssl_context = get_ssl_context (), ) stream_engine = create_engine ( title = \"my-stream-engine\" , backend = backend ) Note Check create ssl context util Example Create SSL context from memory import ssl from kstreams.backends.kafka import Kafka from kstreams import create_engine , utils , Stream def get_ssl_context () -> ssl . SSLContext : return utils . create_ssl_context_from_mem ( cadata = \"ca-certificates-as-unicode\" , certdata = \"client-certificate-as-unicode\" , keydata = \"client-private-key-as-unicode\" , password = \"optional-password-to-load-certificate-chain\" , ) backend = Kafka ( bootstrap_servers = [ \"localhost:9094\" ], security_protocol = \"SSL\" , ssl_context = get_ssl_context (), ) stream_engine = create_engine ( title = \"my-stream-engine\" , backend = backend ) Note Check create ssl context from memerory util Source code in kstreams/backends/kafka.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 class Kafka ( BaseModel ): \"\"\" The `Kafka` backend validates the given attributes. It uses pydantic internally. Attributes: bootstrap_servers: kafka list of `hostname:port` security_protocol: Protocol used to communicate with brokers ssl_context: a python std `ssl.SSLContext` instance, you can generate it with `create_ssl_context` or `create_ssl_context_from_mem` sasl_mechanism: Authentication mechanism when `security_protocol` is configured for `SASL_PLAINTEXT` or `SASL_SSL` sasl_plain_username: username for sasl PLAIN authentication sasl_plain_password: password for sasl PLAIN authentication sasl_oauth_token_provider: smth Raises: ValidationError: a `pydantic.ValidationError` exception ## PLAINTEXT !!! Example ```python from kstreams.backends.kafka import Kafka from kstreams import create_engine, Stream backend = Kafka(bootstrap_servers=[\"localhost:9092\"]) stream_engine = create_engine(title=\"my-stream-engine\", backend=backend) ``` ## SSL !!! Example ```python title=\"Create SSL context\" import ssl from kstreams.backends.kafka import Kafka from kstreams import create_engine, utils, Stream def get_ssl_context() -> ssl.SSLContext: return utils.create_ssl_context( cafile=\"certificate-authority-file-path\", capath=\"points-to-directory-with-several-ca-certificates\", cadata=\"same-as-cafile-but-ASCII-or-bytes-format\", certfile=\"client-certificate-file-name\", keyfile=\"client-private-key-file-name\", password=\"password-to-load-certificate-chain\", ) backend = Kafka( bootstrap_servers=[\"localhost:9094\"], security_protocol=\"SSL\", ssl_context=get_ssl_context(), ) stream_engine = create_engine(title=\"my-stream-engine\", backend=backend) ``` !!! note Check [create ssl context util](https://kpn.github.io/kstreams/utils/#kstreams.utils.create_ssl_context) !!! Example ```python title=\"Create SSL context from memory\" import ssl from kstreams.backends.kafka import Kafka from kstreams import create_engine, utils, Stream def get_ssl_context() -> ssl.SSLContext: return utils.create_ssl_context_from_mem( cadata=\"ca-certificates-as-unicode\", certdata=\"client-certificate-as-unicode\", keydata=\"client-private-key-as-unicode\", password=\"optional-password-to-load-certificate-chain\", ) backend = Kafka( bootstrap_servers=[\"localhost:9094\"], security_protocol=\"SSL\", ssl_context=get_ssl_context(), ) stream_engine = create_engine(title=\"my-stream-engine\", backend=backend) ``` !!! note Check [create ssl context from memerory util](https://kpn.github.io/kstreams/utils/#kstreams.utils.create_ssl_context_from_mem) \"\"\" bootstrap_servers : List [ str ] = [ \"localhost:9092\" ] security_protocol : SecurityProtocol = SecurityProtocol . PLAINTEXT ssl_context : Optional [ ssl . SSLContext ] = None sasl_mechanism : SaslMechanism = SaslMechanism . PLAIN sasl_plain_username : Optional [ str ] = None sasl_plain_password : Optional [ str ] = None sasl_oauth_token_provider : Optional [ str ] = None model_config = ConfigDict ( arbitrary_types_allowed = True , use_enum_values = True ) @model_validator ( mode = \"after\" ) @classmethod def protocols_validation ( cls , values ): security_protocol = values . security_protocol if security_protocol == SecurityProtocol . PLAINTEXT : return values elif security_protocol == SecurityProtocol . SSL : if values . ssl_context is None : raise ValueError ( \"`ssl_context` is required\" ) return values elif security_protocol == SecurityProtocol . SASL_PLAINTEXT : if values . sasl_mechanism is SaslMechanism . OAUTHBEARER : # We don't perform a username and password check if OAUTHBEARER return values if ( values . sasl_mechanism is SaslMechanism . PLAIN and values . sasl_plain_username is None ): raise ValueError ( \"`sasl_plain_username` is required when using SASL_PLAIN\" ) if ( values . sasl_mechanism is SaslMechanism . PLAIN and values . sasl_plain_password is None ): raise ValueError ( \"`sasl_plain_password` is required when using SASL_PLAIN\" ) return values elif security_protocol == SecurityProtocol . SASL_SSL : if values . ssl_context is None : raise ValueError ( \"`ssl_context` is required\" ) if ( values . sasl_mechanism is SaslMechanism . PLAIN and values . sasl_plain_username is None ): raise ValueError ( \"`sasl_plain_username` is required when using SASL_PLAIN\" ) if ( values . sasl_mechanism is SaslMechanism . PLAIN and values . sasl_plain_password is None ): raise ValueError ( \"`sasl_plain_password` is required when using SASL_PLAIN\" ) return values","title":"Backends"},{"location":"backends/#backends","text":"The main idea of a backend is to supply the necessary configuration to create a connection with the backend. kstreams currently has support for Kafka as a backend.","title":"Backends"},{"location":"backends/#kstreams.backends.kafka.Kafka","text":"The Kafka backend validates the given attributes. It uses pydantic internally. Attributes: Name Type Description bootstrap_servers List [ str ] kafka list of hostname:port security_protocol SecurityProtocol Protocol used to communicate with brokers ssl_context Optional [ SSLContext ] a python std ssl.SSLContext instance, you can generate it with create_ssl_context or create_ssl_context_from_mem sasl_mechanism SaslMechanism Authentication mechanism when security_protocol is configured for SASL_PLAINTEXT or SASL_SSL sasl_plain_username Optional [ str ] username for sasl PLAIN authentication sasl_plain_password Optional [ str ] password for sasl PLAIN authentication sasl_oauth_token_provider Optional [ str ] smth Raises: Type Description ValidationError a pydantic.ValidationError exception","title":"Kafka"},{"location":"backends/#kstreams.backends.kafka.Kafka--plaintext","text":"Example from kstreams.backends.kafka import Kafka from kstreams import create_engine , Stream backend = Kafka ( bootstrap_servers = [ \"localhost:9092\" ]) stream_engine = create_engine ( title = \"my-stream-engine\" , backend = backend )","title":"PLAINTEXT"},{"location":"backends/#kstreams.backends.kafka.Kafka--ssl","text":"Example Create SSL context import ssl from kstreams.backends.kafka import Kafka from kstreams import create_engine , utils , Stream def get_ssl_context () -> ssl . SSLContext : return utils . create_ssl_context ( cafile = \"certificate-authority-file-path\" , capath = \"points-to-directory-with-several-ca-certificates\" , cadata = \"same-as-cafile-but-ASCII-or-bytes-format\" , certfile = \"client-certificate-file-name\" , keyfile = \"client-private-key-file-name\" , password = \"password-to-load-certificate-chain\" , ) backend = Kafka ( bootstrap_servers = [ \"localhost:9094\" ], security_protocol = \"SSL\" , ssl_context = get_ssl_context (), ) stream_engine = create_engine ( title = \"my-stream-engine\" , backend = backend ) Note Check create ssl context util Example Create SSL context from memory import ssl from kstreams.backends.kafka import Kafka from kstreams import create_engine , utils , Stream def get_ssl_context () -> ssl . SSLContext : return utils . create_ssl_context_from_mem ( cadata = \"ca-certificates-as-unicode\" , certdata = \"client-certificate-as-unicode\" , keydata = \"client-private-key-as-unicode\" , password = \"optional-password-to-load-certificate-chain\" , ) backend = Kafka ( bootstrap_servers = [ \"localhost:9094\" ], security_protocol = \"SSL\" , ssl_context = get_ssl_context (), ) stream_engine = create_engine ( title = \"my-stream-engine\" , backend = backend ) Note Check create ssl context from memerory util Source code in kstreams/backends/kafka.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 class Kafka ( BaseModel ): \"\"\" The `Kafka` backend validates the given attributes. It uses pydantic internally. Attributes: bootstrap_servers: kafka list of `hostname:port` security_protocol: Protocol used to communicate with brokers ssl_context: a python std `ssl.SSLContext` instance, you can generate it with `create_ssl_context` or `create_ssl_context_from_mem` sasl_mechanism: Authentication mechanism when `security_protocol` is configured for `SASL_PLAINTEXT` or `SASL_SSL` sasl_plain_username: username for sasl PLAIN authentication sasl_plain_password: password for sasl PLAIN authentication sasl_oauth_token_provider: smth Raises: ValidationError: a `pydantic.ValidationError` exception ## PLAINTEXT !!! Example ```python from kstreams.backends.kafka import Kafka from kstreams import create_engine, Stream backend = Kafka(bootstrap_servers=[\"localhost:9092\"]) stream_engine = create_engine(title=\"my-stream-engine\", backend=backend) ``` ## SSL !!! Example ```python title=\"Create SSL context\" import ssl from kstreams.backends.kafka import Kafka from kstreams import create_engine, utils, Stream def get_ssl_context() -> ssl.SSLContext: return utils.create_ssl_context( cafile=\"certificate-authority-file-path\", capath=\"points-to-directory-with-several-ca-certificates\", cadata=\"same-as-cafile-but-ASCII-or-bytes-format\", certfile=\"client-certificate-file-name\", keyfile=\"client-private-key-file-name\", password=\"password-to-load-certificate-chain\", ) backend = Kafka( bootstrap_servers=[\"localhost:9094\"], security_protocol=\"SSL\", ssl_context=get_ssl_context(), ) stream_engine = create_engine(title=\"my-stream-engine\", backend=backend) ``` !!! note Check [create ssl context util](https://kpn.github.io/kstreams/utils/#kstreams.utils.create_ssl_context) !!! Example ```python title=\"Create SSL context from memory\" import ssl from kstreams.backends.kafka import Kafka from kstreams import create_engine, utils, Stream def get_ssl_context() -> ssl.SSLContext: return utils.create_ssl_context_from_mem( cadata=\"ca-certificates-as-unicode\", certdata=\"client-certificate-as-unicode\", keydata=\"client-private-key-as-unicode\", password=\"optional-password-to-load-certificate-chain\", ) backend = Kafka( bootstrap_servers=[\"localhost:9094\"], security_protocol=\"SSL\", ssl_context=get_ssl_context(), ) stream_engine = create_engine(title=\"my-stream-engine\", backend=backend) ``` !!! note Check [create ssl context from memerory util](https://kpn.github.io/kstreams/utils/#kstreams.utils.create_ssl_context_from_mem) \"\"\" bootstrap_servers : List [ str ] = [ \"localhost:9092\" ] security_protocol : SecurityProtocol = SecurityProtocol . PLAINTEXT ssl_context : Optional [ ssl . SSLContext ] = None sasl_mechanism : SaslMechanism = SaslMechanism . PLAIN sasl_plain_username : Optional [ str ] = None sasl_plain_password : Optional [ str ] = None sasl_oauth_token_provider : Optional [ str ] = None model_config = ConfigDict ( arbitrary_types_allowed = True , use_enum_values = True ) @model_validator ( mode = \"after\" ) @classmethod def protocols_validation ( cls , values ): security_protocol = values . security_protocol if security_protocol == SecurityProtocol . PLAINTEXT : return values elif security_protocol == SecurityProtocol . SSL : if values . ssl_context is None : raise ValueError ( \"`ssl_context` is required\" ) return values elif security_protocol == SecurityProtocol . SASL_PLAINTEXT : if values . sasl_mechanism is SaslMechanism . OAUTHBEARER : # We don't perform a username and password check if OAUTHBEARER return values if ( values . sasl_mechanism is SaslMechanism . PLAIN and values . sasl_plain_username is None ): raise ValueError ( \"`sasl_plain_username` is required when using SASL_PLAIN\" ) if ( values . sasl_mechanism is SaslMechanism . PLAIN and values . sasl_plain_password is None ): raise ValueError ( \"`sasl_plain_password` is required when using SASL_PLAIN\" ) return values elif security_protocol == SecurityProtocol . SASL_SSL : if values . ssl_context is None : raise ValueError ( \"`ssl_context` is required\" ) if ( values . sasl_mechanism is SaslMechanism . PLAIN and values . sasl_plain_username is None ): raise ValueError ( \"`sasl_plain_username` is required when using SASL_PLAIN\" ) if ( values . sasl_mechanism is SaslMechanism . PLAIN and values . sasl_plain_password is None ): raise ValueError ( \"`sasl_plain_password` is required when using SASL_PLAIN\" ) return values","title":"SSL"},{"location":"engine/","text":"StreamEngine kstreams.engine.StreamEngine Attributes: Name Type Description backend Kafka Backend to connect. Default Kafka consumer_class Consumer The consumer class to use when instanciate a consumer. Default kstreams.Consumer producer_class Producer The producer class to use when instanciate the producer. Default kstreams.Producer monitor PrometheusMonitor Prometheus monitor that holds the metrics title str | None Engine name serializer Serializer | None Serializer to use when an event is produced. deserializer Deserializer | None Deserializer to be used when an event is consumed. If provided it will be used in all Streams instances as a general one. To override it per Stream, you can provide one per Stream Example Usage import kstreams stream_engine = kstreams . create_engine ( title = \"my-stream-engine\" ) @kstreams . stream ( \"local--hello-world\" , group_id = \"example-group\" ) async def consume ( stream : kstreams . ConsumerRecord ) -> None : print ( f \"showing bytes: { cr . value } \" ) await stream_engine . start () Source code in kstreams/engine.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 class StreamEngine : \"\"\" Attributes: backend kstreams.backends.Kafka: Backend to connect. Default `Kafka` consumer_class kstreams.Consumer: The consumer class to use when instanciate a consumer. Default kstreams.Consumer producer_class kstreams.Producer: The producer class to use when instanciate the producer. Default kstreams.Producer monitor kstreams.PrometheusMonitor: Prometheus monitor that holds the [metrics](https://kpn.github.io/kstreams/metrics/) title str | None: Engine name serializer kstreams.serializers.Serializer | None: Serializer to use when an event is produced. deserializer kstreams.serializers.Deserializer | None: Deserializer to be used when an event is consumed. If provided it will be used in all Streams instances as a general one. To override it per Stream, you can provide one per Stream !!! Example ```python title=\"Usage\" import kstreams stream_engine = kstreams.create_engine( title=\"my-stream-engine\" ) @kstreams.stream(\"local--hello-world\", group_id=\"example-group\") async def consume(stream: kstreams.ConsumerRecord) -> None: print(f\"showing bytes: {cr.value}\") await stream_engine.start() ``` \"\"\" def __init__ ( self , * , backend : Kafka , consumer_class : typing . Type [ Consumer ], producer_class : typing . Type [ Producer ], monitor : PrometheusMonitor , title : typing . Optional [ str ] = None , deserializer : typing . Optional [ Deserializer ] = None , serializer : typing . Optional [ Serializer ] = None , on_startup : typing . Optional [ EngineHooks ] = None , on_stop : typing . Optional [ EngineHooks ] = None , after_startup : typing . Optional [ EngineHooks ] = None , after_stop : typing . Optional [ EngineHooks ] = None , ) -> None : self . title = title self . backend = backend self . consumer_class = consumer_class self . producer_class = producer_class self . deserializer = deserializer self . serializer = serializer self . monitor = monitor self . _producer : typing . Optional [ typing . Type [ Producer ]] = None self . _streams : typing . List [ Stream ] = [] self . _on_startup = [] if on_startup is None else list ( on_startup ) self . _on_stop = [] if on_stop is None else list ( on_stop ) self . _after_startup = [] if after_startup is None else list ( after_startup ) self . _after_stop = [] if after_stop is None else list ( after_stop ) async def send ( self , topic : str , value : typing . Any = None , key : typing . Any = None , partition : typing . Optional [ int ] = None , timestamp_ms : typing . Optional [ int ] = None , headers : typing . Optional [ Headers ] = None , serializer : typing . Optional [ Serializer ] = None , serializer_kwargs : typing . Optional [ typing . Dict ] = None , ): \"\"\" Attributes: topic str: Topic name to send the event to value Any: Event value key str | None: Event key partition int | None: Topic partition timestamp_ms int | None: Event timestamp in miliseconds headers Dict[str, str] | None: Event headers serializer kstreams.serializers.Serializer | None: Serializer to encode the event serializer_kwargs Dict[str, Any] | None: Serializer kwargs \"\"\" if self . _producer is None : raise EngineNotStartedException () serializer = serializer or self . serializer # serialize only when value and serializer are present if value is not None and serializer is not None : value = await serializer . serialize ( value , headers = headers , serializer_kwargs = serializer_kwargs ) encoded_headers = None if headers is not None : encoded_headers = encode_headers ( headers ) fut = await self . _producer . send ( topic , value = value , key = key , partition = partition , timestamp_ms = timestamp_ms , headers = encoded_headers , ) metadata : RecordMetadata = await fut self . monitor . add_topic_partition_offset ( topic , metadata . partition , metadata . offset ) return metadata async def start ( self ) -> None : # Execute on_startup hooks await execute_hooks ( self . _on_startup ) # add the producer and streams to the Monitor self . monitor . add_producer ( self . _producer ) self . monitor . add_streams ( self . _streams ) await self . start_producer () await self . start_streams () # Execute after_startup hooks await execute_hooks ( self . _after_startup ) def on_startup ( self , func : typing . Callable [[], typing . Any ], ) -> typing . Callable [[], typing . Any ]: \"\"\" A list of callables to run before the engine starts. Handler are callables that do not take any arguments, and may be either standard functions, or async functions. Attributes: func typing.Callable[[], typing.Any]: Func to callable before engine starts !!! Example ```python title=\"Engine before startup\" import kstreams stream_engine = kstreams.create_engine( title=\"my-stream-engine\" ) @stream_engine.on_startup async def init_db() -> None: print(\"Initializing Database Connections\") await init_db() @stream_engine.on_startup async def start_background_task() -> None: print(\"Some background task\") ``` \"\"\" self . _on_startup . append ( func ) return func def on_stop ( self , func : typing . Callable [[], typing . Any ], ) -> typing . Callable [[], typing . Any ]: \"\"\" A list of callables to run before the engine stops. Handler are callables that do not take any arguments, and may be either standard functions, or async functions. Attributes: func typing.Callable[[], typing.Any]: Func to callable before engine stops !!! Example ```python title=\"Engine before stops\" import kstreams stream_engine = kstreams.create_engine( title=\"my-stream-engine\" ) @stream_engine.on_stop async def close_db() -> None: print(\"Closing Database Connections\") await db_close() ``` \"\"\" self . _on_stop . append ( func ) return func def after_startup ( self , func : typing . Callable [[], typing . Any ], ) -> typing . Callable [[], typing . Any ]: \"\"\" A list of callables to run after the engine starts. Handler are callables that do not take any arguments, and may be either standard functions, or async functions. Attributes: func typing.Callable[[], typing.Any]: Func to callable after engine starts !!! Example ```python title=\"Engine after startup\" import kstreams stream_engine = kstreams.create_engine( title=\"my-stream-engine\" ) @stream_engine.after_startup async def after_startup() -> None: print(\"Set pod as healthy\") await mark_healthy_pod() ``` \"\"\" self . _after_startup . append ( func ) return func def after_stop ( self , func : typing . Callable [[], typing . Any ], ) -> typing . Callable [[], typing . Any ]: \"\"\" A list of callables to run after the engine stops. Handler are callables that do not take any arguments, and may be either standard functions, or async functions. Attributes: func typing.Callable[[], typing.Any]: Func to callable after engine stops !!! Example ```python title=\"Engine after stops\" import kstreams stream_engine = kstreams.create_engine( title=\"my-stream-engine\" ) @stream_engine.after_stop async def after_stop() -> None: print(\"Finishing backgrpund tasks\") ``` \"\"\" self . _after_stop . append ( func ) return func async def stop ( self ) -> None : # Execute on_startup hooks await execute_hooks ( self . _on_stop ) await self . monitor . stop () await self . stop_producer () await self . stop_streams () # Execute after_startup hooks await execute_hooks ( self . _after_stop ) async def stop_producer ( self ): if self . _producer is not None : await self . _producer . stop () logger . info ( \"Producer has STOPPED....\" ) async def start_producer ( self , ** kwargs ) -> None : if self . producer_class is None : return None config = { ** self . backend . model_dump (), ** kwargs } self . _producer = self . producer_class ( ** config ) if self . _producer is None : return None await self . _producer . start () async def start_streams ( self ) -> None : # Only start the Streams that are not async_generators streams = [ stream for stream in self . _streams if not inspect . isasyncgenfunction ( stream . func ) ] await self . _start_streams_on_background_mode ( streams ) async def _start_streams_on_background_mode ( self , streams : typing . List [ Stream ] ) -> None : # start all the streams for stream in streams : asyncio . create_task ( stream . start ()) # start monitoring asyncio . create_task ( self . monitor . start ()) async def stop_streams ( self ) -> None : for stream in self . _streams : await stream . stop () logger . info ( \"Streams have STOPPED....\" ) async def clean_streams ( self ): await self . stop_streams () self . _streams = [] def exist_stream ( self , name : str ) -> bool : stream = self . get_stream ( name ) return True if stream is not None else False def get_stream ( self , name : str ) -> typing . Optional [ Stream ]: stream = next (( stream for stream in self . _streams if stream . name == name ), None ) return stream def add_stream ( self , stream : Stream , error_policy : StreamErrorPolicy = StreamErrorPolicy . STOP ) -> None : if self . exist_stream ( stream . name ): raise DuplicateStreamException ( name = stream . name ) stream . backend = self . backend if stream . deserializer is None : stream . deserializer = self . deserializer self . _streams . append ( stream ) if stream . rebalance_listener is None : # set the stream to the listener to it will be available # when the callbacks are called stream . rebalance_listener = MetricsRebalanceListener () stream . rebalance_listener . stream = stream # type: ignore stream . rebalance_listener . engine = self # type: ignore stream . udf_handler = UdfHandler ( next_call = stream . func , send = self . send , stream = stream , ) # NOTE: When `no typing` support is deprecated this check can # be removed if stream . udf_handler . type != UDFType . NO_TYPING : stream . func = self . build_stream_middleware_stack ( stream = stream , error_policy = error_policy ) def build_stream_middleware_stack ( self , * , stream : Stream , error_policy : StreamErrorPolicy ) -> NextMiddlewareCall : assert stream . udf_handler , \"UdfHandler can not be None\" stream . middlewares = [ Middleware ( ExceptionMiddleware , engine = self , error_policy = error_policy ), ] + stream . middlewares next_call = stream . udf_handler for middleware , options in reversed ( stream . middlewares ): next_call = middleware ( next_call = next_call , send = self . send , stream = stream , ** options ) return next_call async def remove_stream ( self , stream : Stream ) -> None : consumer = stream . consumer self . _streams . remove ( stream ) await stream . stop () if consumer is not None : self . monitor . clean_stream_consumer_metrics ( consumer = consumer ) def stream ( self , topics : typing . Union [ typing . List [ str ], str ], * , name : typing . Optional [ str ] = None , deserializer : typing . Optional [ Deserializer ] = None , initial_offsets : typing . Optional [ typing . List [ TopicPartitionOffset ]] = None , rebalance_listener : typing . Optional [ RebalanceListener ] = None , middlewares : typing . Optional [ typing . List [ Middleware ]] = None , subscribe_by_pattern : bool = False , error_policy : StreamErrorPolicy = StreamErrorPolicy . STOP , ** kwargs , ) -> typing . Callable [[ StreamFunc ], Stream ]: def decorator ( func : StreamFunc ) -> Stream : stream_from_func = stream_func ( topics , name = name , deserializer = deserializer , initial_offsets = initial_offsets , rebalance_listener = rebalance_listener , middlewares = middlewares , subscribe_by_pattern = subscribe_by_pattern , ** kwargs , )( func ) self . add_stream ( stream_from_func , error_policy = error_policy ) return stream_from_func return decorator send ( topic , value = None , key = None , partition = None , timestamp_ms = None , headers = None , serializer = None , serializer_kwargs = None ) async Attributes: Name Type Description topic str Topic name to send the event to value Any Event value key str | None Event key partition int | None Topic partition timestamp_ms int | None Event timestamp in miliseconds headers Dict [ str , str ] | None Event headers serializer Serializer | None Serializer to encode the event serializer_kwargs Dict [ str , Any ] | None Serializer kwargs Source code in kstreams/engine.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 async def send ( self , topic : str , value : typing . Any = None , key : typing . Any = None , partition : typing . Optional [ int ] = None , timestamp_ms : typing . Optional [ int ] = None , headers : typing . Optional [ Headers ] = None , serializer : typing . Optional [ Serializer ] = None , serializer_kwargs : typing . Optional [ typing . Dict ] = None , ): \"\"\" Attributes: topic str: Topic name to send the event to value Any: Event value key str | None: Event key partition int | None: Topic partition timestamp_ms int | None: Event timestamp in miliseconds headers Dict[str, str] | None: Event headers serializer kstreams.serializers.Serializer | None: Serializer to encode the event serializer_kwargs Dict[str, Any] | None: Serializer kwargs \"\"\" if self . _producer is None : raise EngineNotStartedException () serializer = serializer or self . serializer # serialize only when value and serializer are present if value is not None and serializer is not None : value = await serializer . serialize ( value , headers = headers , serializer_kwargs = serializer_kwargs ) encoded_headers = None if headers is not None : encoded_headers = encode_headers ( headers ) fut = await self . _producer . send ( topic , value = value , key = key , partition = partition , timestamp_ms = timestamp_ms , headers = encoded_headers , ) metadata : RecordMetadata = await fut self . monitor . add_topic_partition_offset ( topic , metadata . partition , metadata . offset ) return metadata on_startup ( func ) A list of callables to run before the engine starts. Handler are callables that do not take any arguments, and may be either standard functions, or async functions. Attributes: Name Type Description func Callable [[], Any ] Func to callable before engine starts Example Engine before startup import kstreams stream_engine = kstreams . create_engine ( title = \"my-stream-engine\" ) @stream_engine . on_startup async def init_db () -> None : print ( \"Initializing Database Connections\" ) await init_db () @stream_engine . on_startup async def start_background_task () -> None : print ( \"Some background task\" ) Source code in kstreams/engine.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def on_startup ( self , func : typing . Callable [[], typing . Any ], ) -> typing . Callable [[], typing . Any ]: \"\"\" A list of callables to run before the engine starts. Handler are callables that do not take any arguments, and may be either standard functions, or async functions. Attributes: func typing.Callable[[], typing.Any]: Func to callable before engine starts !!! Example ```python title=\"Engine before startup\" import kstreams stream_engine = kstreams.create_engine( title=\"my-stream-engine\" ) @stream_engine.on_startup async def init_db() -> None: print(\"Initializing Database Connections\") await init_db() @stream_engine.on_startup async def start_background_task() -> None: print(\"Some background task\") ``` \"\"\" self . _on_startup . append ( func ) return func on_stop ( func ) A list of callables to run before the engine stops. Handler are callables that do not take any arguments, and may be either standard functions, or async functions. Attributes: Name Type Description func Callable [[], Any ] Func to callable before engine stops Example Engine before stops import kstreams stream_engine = kstreams . create_engine ( title = \"my-stream-engine\" ) @stream_engine . on_stop async def close_db () -> None : print ( \"Closing Database Connections\" ) await db_close () Source code in kstreams/engine.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def on_stop ( self , func : typing . Callable [[], typing . Any ], ) -> typing . Callable [[], typing . Any ]: \"\"\" A list of callables to run before the engine stops. Handler are callables that do not take any arguments, and may be either standard functions, or async functions. Attributes: func typing.Callable[[], typing.Any]: Func to callable before engine stops !!! Example ```python title=\"Engine before stops\" import kstreams stream_engine = kstreams.create_engine( title=\"my-stream-engine\" ) @stream_engine.on_stop async def close_db() -> None: print(\"Closing Database Connections\") await db_close() ``` \"\"\" self . _on_stop . append ( func ) return func after_startup ( func ) A list of callables to run after the engine starts. Handler are callables that do not take any arguments, and may be either standard functions, or async functions. Attributes: Name Type Description func Callable [[], Any ] Func to callable after engine starts Example Engine after startup import kstreams stream_engine = kstreams . create_engine ( title = \"my-stream-engine\" ) @stream_engine . after_startup async def after_startup () -> None : print ( \"Set pod as healthy\" ) await mark_healthy_pod () Source code in kstreams/engine.py 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 def after_startup ( self , func : typing . Callable [[], typing . Any ], ) -> typing . Callable [[], typing . Any ]: \"\"\" A list of callables to run after the engine starts. Handler are callables that do not take any arguments, and may be either standard functions, or async functions. Attributes: func typing.Callable[[], typing.Any]: Func to callable after engine starts !!! Example ```python title=\"Engine after startup\" import kstreams stream_engine = kstreams.create_engine( title=\"my-stream-engine\" ) @stream_engine.after_startup async def after_startup() -> None: print(\"Set pod as healthy\") await mark_healthy_pod() ``` \"\"\" self . _after_startup . append ( func ) return func after_stop ( func ) A list of callables to run after the engine stops. Handler are callables that do not take any arguments, and may be either standard functions, or async functions. Attributes: Name Type Description func Callable [[], Any ] Func to callable after engine stops Example Engine after stops import kstreams stream_engine = kstreams . create_engine ( title = \"my-stream-engine\" ) @stream_engine . after_stop async def after_stop () -> None : print ( \"Finishing backgrpund tasks\" ) Source code in kstreams/engine.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 def after_stop ( self , func : typing . Callable [[], typing . Any ], ) -> typing . Callable [[], typing . Any ]: \"\"\" A list of callables to run after the engine stops. Handler are callables that do not take any arguments, and may be either standard functions, or async functions. Attributes: func typing.Callable[[], typing.Any]: Func to callable after engine stops !!! Example ```python title=\"Engine after stops\" import kstreams stream_engine = kstreams.create_engine( title=\"my-stream-engine\" ) @stream_engine.after_stop async def after_stop() -> None: print(\"Finishing backgrpund tasks\") ``` \"\"\" self . _after_stop . append ( func ) return func","title":"StreamEngine"},{"location":"engine/#streamengine","text":"","title":"StreamEngine"},{"location":"engine/#kstreams.engine.StreamEngine","text":"Attributes: Name Type Description backend Kafka Backend to connect. Default Kafka consumer_class Consumer The consumer class to use when instanciate a consumer. Default kstreams.Consumer producer_class Producer The producer class to use when instanciate the producer. Default kstreams.Producer monitor PrometheusMonitor Prometheus monitor that holds the metrics title str | None Engine name serializer Serializer | None Serializer to use when an event is produced. deserializer Deserializer | None Deserializer to be used when an event is consumed. If provided it will be used in all Streams instances as a general one. To override it per Stream, you can provide one per Stream Example Usage import kstreams stream_engine = kstreams . create_engine ( title = \"my-stream-engine\" ) @kstreams . stream ( \"local--hello-world\" , group_id = \"example-group\" ) async def consume ( stream : kstreams . ConsumerRecord ) -> None : print ( f \"showing bytes: { cr . value } \" ) await stream_engine . start () Source code in kstreams/engine.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 class StreamEngine : \"\"\" Attributes: backend kstreams.backends.Kafka: Backend to connect. Default `Kafka` consumer_class kstreams.Consumer: The consumer class to use when instanciate a consumer. Default kstreams.Consumer producer_class kstreams.Producer: The producer class to use when instanciate the producer. Default kstreams.Producer monitor kstreams.PrometheusMonitor: Prometheus monitor that holds the [metrics](https://kpn.github.io/kstreams/metrics/) title str | None: Engine name serializer kstreams.serializers.Serializer | None: Serializer to use when an event is produced. deserializer kstreams.serializers.Deserializer | None: Deserializer to be used when an event is consumed. If provided it will be used in all Streams instances as a general one. To override it per Stream, you can provide one per Stream !!! Example ```python title=\"Usage\" import kstreams stream_engine = kstreams.create_engine( title=\"my-stream-engine\" ) @kstreams.stream(\"local--hello-world\", group_id=\"example-group\") async def consume(stream: kstreams.ConsumerRecord) -> None: print(f\"showing bytes: {cr.value}\") await stream_engine.start() ``` \"\"\" def __init__ ( self , * , backend : Kafka , consumer_class : typing . Type [ Consumer ], producer_class : typing . Type [ Producer ], monitor : PrometheusMonitor , title : typing . Optional [ str ] = None , deserializer : typing . Optional [ Deserializer ] = None , serializer : typing . Optional [ Serializer ] = None , on_startup : typing . Optional [ EngineHooks ] = None , on_stop : typing . Optional [ EngineHooks ] = None , after_startup : typing . Optional [ EngineHooks ] = None , after_stop : typing . Optional [ EngineHooks ] = None , ) -> None : self . title = title self . backend = backend self . consumer_class = consumer_class self . producer_class = producer_class self . deserializer = deserializer self . serializer = serializer self . monitor = monitor self . _producer : typing . Optional [ typing . Type [ Producer ]] = None self . _streams : typing . List [ Stream ] = [] self . _on_startup = [] if on_startup is None else list ( on_startup ) self . _on_stop = [] if on_stop is None else list ( on_stop ) self . _after_startup = [] if after_startup is None else list ( after_startup ) self . _after_stop = [] if after_stop is None else list ( after_stop ) async def send ( self , topic : str , value : typing . Any = None , key : typing . Any = None , partition : typing . Optional [ int ] = None , timestamp_ms : typing . Optional [ int ] = None , headers : typing . Optional [ Headers ] = None , serializer : typing . Optional [ Serializer ] = None , serializer_kwargs : typing . Optional [ typing . Dict ] = None , ): \"\"\" Attributes: topic str: Topic name to send the event to value Any: Event value key str | None: Event key partition int | None: Topic partition timestamp_ms int | None: Event timestamp in miliseconds headers Dict[str, str] | None: Event headers serializer kstreams.serializers.Serializer | None: Serializer to encode the event serializer_kwargs Dict[str, Any] | None: Serializer kwargs \"\"\" if self . _producer is None : raise EngineNotStartedException () serializer = serializer or self . serializer # serialize only when value and serializer are present if value is not None and serializer is not None : value = await serializer . serialize ( value , headers = headers , serializer_kwargs = serializer_kwargs ) encoded_headers = None if headers is not None : encoded_headers = encode_headers ( headers ) fut = await self . _producer . send ( topic , value = value , key = key , partition = partition , timestamp_ms = timestamp_ms , headers = encoded_headers , ) metadata : RecordMetadata = await fut self . monitor . add_topic_partition_offset ( topic , metadata . partition , metadata . offset ) return metadata async def start ( self ) -> None : # Execute on_startup hooks await execute_hooks ( self . _on_startup ) # add the producer and streams to the Monitor self . monitor . add_producer ( self . _producer ) self . monitor . add_streams ( self . _streams ) await self . start_producer () await self . start_streams () # Execute after_startup hooks await execute_hooks ( self . _after_startup ) def on_startup ( self , func : typing . Callable [[], typing . Any ], ) -> typing . Callable [[], typing . Any ]: \"\"\" A list of callables to run before the engine starts. Handler are callables that do not take any arguments, and may be either standard functions, or async functions. Attributes: func typing.Callable[[], typing.Any]: Func to callable before engine starts !!! Example ```python title=\"Engine before startup\" import kstreams stream_engine = kstreams.create_engine( title=\"my-stream-engine\" ) @stream_engine.on_startup async def init_db() -> None: print(\"Initializing Database Connections\") await init_db() @stream_engine.on_startup async def start_background_task() -> None: print(\"Some background task\") ``` \"\"\" self . _on_startup . append ( func ) return func def on_stop ( self , func : typing . Callable [[], typing . Any ], ) -> typing . Callable [[], typing . Any ]: \"\"\" A list of callables to run before the engine stops. Handler are callables that do not take any arguments, and may be either standard functions, or async functions. Attributes: func typing.Callable[[], typing.Any]: Func to callable before engine stops !!! Example ```python title=\"Engine before stops\" import kstreams stream_engine = kstreams.create_engine( title=\"my-stream-engine\" ) @stream_engine.on_stop async def close_db() -> None: print(\"Closing Database Connections\") await db_close() ``` \"\"\" self . _on_stop . append ( func ) return func def after_startup ( self , func : typing . Callable [[], typing . Any ], ) -> typing . Callable [[], typing . Any ]: \"\"\" A list of callables to run after the engine starts. Handler are callables that do not take any arguments, and may be either standard functions, or async functions. Attributes: func typing.Callable[[], typing.Any]: Func to callable after engine starts !!! Example ```python title=\"Engine after startup\" import kstreams stream_engine = kstreams.create_engine( title=\"my-stream-engine\" ) @stream_engine.after_startup async def after_startup() -> None: print(\"Set pod as healthy\") await mark_healthy_pod() ``` \"\"\" self . _after_startup . append ( func ) return func def after_stop ( self , func : typing . Callable [[], typing . Any ], ) -> typing . Callable [[], typing . Any ]: \"\"\" A list of callables to run after the engine stops. Handler are callables that do not take any arguments, and may be either standard functions, or async functions. Attributes: func typing.Callable[[], typing.Any]: Func to callable after engine stops !!! Example ```python title=\"Engine after stops\" import kstreams stream_engine = kstreams.create_engine( title=\"my-stream-engine\" ) @stream_engine.after_stop async def after_stop() -> None: print(\"Finishing backgrpund tasks\") ``` \"\"\" self . _after_stop . append ( func ) return func async def stop ( self ) -> None : # Execute on_startup hooks await execute_hooks ( self . _on_stop ) await self . monitor . stop () await self . stop_producer () await self . stop_streams () # Execute after_startup hooks await execute_hooks ( self . _after_stop ) async def stop_producer ( self ): if self . _producer is not None : await self . _producer . stop () logger . info ( \"Producer has STOPPED....\" ) async def start_producer ( self , ** kwargs ) -> None : if self . producer_class is None : return None config = { ** self . backend . model_dump (), ** kwargs } self . _producer = self . producer_class ( ** config ) if self . _producer is None : return None await self . _producer . start () async def start_streams ( self ) -> None : # Only start the Streams that are not async_generators streams = [ stream for stream in self . _streams if not inspect . isasyncgenfunction ( stream . func ) ] await self . _start_streams_on_background_mode ( streams ) async def _start_streams_on_background_mode ( self , streams : typing . List [ Stream ] ) -> None : # start all the streams for stream in streams : asyncio . create_task ( stream . start ()) # start monitoring asyncio . create_task ( self . monitor . start ()) async def stop_streams ( self ) -> None : for stream in self . _streams : await stream . stop () logger . info ( \"Streams have STOPPED....\" ) async def clean_streams ( self ): await self . stop_streams () self . _streams = [] def exist_stream ( self , name : str ) -> bool : stream = self . get_stream ( name ) return True if stream is not None else False def get_stream ( self , name : str ) -> typing . Optional [ Stream ]: stream = next (( stream for stream in self . _streams if stream . name == name ), None ) return stream def add_stream ( self , stream : Stream , error_policy : StreamErrorPolicy = StreamErrorPolicy . STOP ) -> None : if self . exist_stream ( stream . name ): raise DuplicateStreamException ( name = stream . name ) stream . backend = self . backend if stream . deserializer is None : stream . deserializer = self . deserializer self . _streams . append ( stream ) if stream . rebalance_listener is None : # set the stream to the listener to it will be available # when the callbacks are called stream . rebalance_listener = MetricsRebalanceListener () stream . rebalance_listener . stream = stream # type: ignore stream . rebalance_listener . engine = self # type: ignore stream . udf_handler = UdfHandler ( next_call = stream . func , send = self . send , stream = stream , ) # NOTE: When `no typing` support is deprecated this check can # be removed if stream . udf_handler . type != UDFType . NO_TYPING : stream . func = self . build_stream_middleware_stack ( stream = stream , error_policy = error_policy ) def build_stream_middleware_stack ( self , * , stream : Stream , error_policy : StreamErrorPolicy ) -> NextMiddlewareCall : assert stream . udf_handler , \"UdfHandler can not be None\" stream . middlewares = [ Middleware ( ExceptionMiddleware , engine = self , error_policy = error_policy ), ] + stream . middlewares next_call = stream . udf_handler for middleware , options in reversed ( stream . middlewares ): next_call = middleware ( next_call = next_call , send = self . send , stream = stream , ** options ) return next_call async def remove_stream ( self , stream : Stream ) -> None : consumer = stream . consumer self . _streams . remove ( stream ) await stream . stop () if consumer is not None : self . monitor . clean_stream_consumer_metrics ( consumer = consumer ) def stream ( self , topics : typing . Union [ typing . List [ str ], str ], * , name : typing . Optional [ str ] = None , deserializer : typing . Optional [ Deserializer ] = None , initial_offsets : typing . Optional [ typing . List [ TopicPartitionOffset ]] = None , rebalance_listener : typing . Optional [ RebalanceListener ] = None , middlewares : typing . Optional [ typing . List [ Middleware ]] = None , subscribe_by_pattern : bool = False , error_policy : StreamErrorPolicy = StreamErrorPolicy . STOP , ** kwargs , ) -> typing . Callable [[ StreamFunc ], Stream ]: def decorator ( func : StreamFunc ) -> Stream : stream_from_func = stream_func ( topics , name = name , deserializer = deserializer , initial_offsets = initial_offsets , rebalance_listener = rebalance_listener , middlewares = middlewares , subscribe_by_pattern = subscribe_by_pattern , ** kwargs , )( func ) self . add_stream ( stream_from_func , error_policy = error_policy ) return stream_from_func return decorator","title":"StreamEngine"},{"location":"engine/#kstreams.engine.StreamEngine.send","text":"Attributes: Name Type Description topic str Topic name to send the event to value Any Event value key str | None Event key partition int | None Topic partition timestamp_ms int | None Event timestamp in miliseconds headers Dict [ str , str ] | None Event headers serializer Serializer | None Serializer to encode the event serializer_kwargs Dict [ str , Any ] | None Serializer kwargs Source code in kstreams/engine.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 async def send ( self , topic : str , value : typing . Any = None , key : typing . Any = None , partition : typing . Optional [ int ] = None , timestamp_ms : typing . Optional [ int ] = None , headers : typing . Optional [ Headers ] = None , serializer : typing . Optional [ Serializer ] = None , serializer_kwargs : typing . Optional [ typing . Dict ] = None , ): \"\"\" Attributes: topic str: Topic name to send the event to value Any: Event value key str | None: Event key partition int | None: Topic partition timestamp_ms int | None: Event timestamp in miliseconds headers Dict[str, str] | None: Event headers serializer kstreams.serializers.Serializer | None: Serializer to encode the event serializer_kwargs Dict[str, Any] | None: Serializer kwargs \"\"\" if self . _producer is None : raise EngineNotStartedException () serializer = serializer or self . serializer # serialize only when value and serializer are present if value is not None and serializer is not None : value = await serializer . serialize ( value , headers = headers , serializer_kwargs = serializer_kwargs ) encoded_headers = None if headers is not None : encoded_headers = encode_headers ( headers ) fut = await self . _producer . send ( topic , value = value , key = key , partition = partition , timestamp_ms = timestamp_ms , headers = encoded_headers , ) metadata : RecordMetadata = await fut self . monitor . add_topic_partition_offset ( topic , metadata . partition , metadata . offset ) return metadata","title":"send()"},{"location":"engine/#kstreams.engine.StreamEngine.on_startup","text":"A list of callables to run before the engine starts. Handler are callables that do not take any arguments, and may be either standard functions, or async functions. Attributes: Name Type Description func Callable [[], Any ] Func to callable before engine starts Example Engine before startup import kstreams stream_engine = kstreams . create_engine ( title = \"my-stream-engine\" ) @stream_engine . on_startup async def init_db () -> None : print ( \"Initializing Database Connections\" ) await init_db () @stream_engine . on_startup async def start_background_task () -> None : print ( \"Some background task\" ) Source code in kstreams/engine.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def on_startup ( self , func : typing . Callable [[], typing . Any ], ) -> typing . Callable [[], typing . Any ]: \"\"\" A list of callables to run before the engine starts. Handler are callables that do not take any arguments, and may be either standard functions, or async functions. Attributes: func typing.Callable[[], typing.Any]: Func to callable before engine starts !!! Example ```python title=\"Engine before startup\" import kstreams stream_engine = kstreams.create_engine( title=\"my-stream-engine\" ) @stream_engine.on_startup async def init_db() -> None: print(\"Initializing Database Connections\") await init_db() @stream_engine.on_startup async def start_background_task() -> None: print(\"Some background task\") ``` \"\"\" self . _on_startup . append ( func ) return func","title":"on_startup()"},{"location":"engine/#kstreams.engine.StreamEngine.on_stop","text":"A list of callables to run before the engine stops. Handler are callables that do not take any arguments, and may be either standard functions, or async functions. Attributes: Name Type Description func Callable [[], Any ] Func to callable before engine stops Example Engine before stops import kstreams stream_engine = kstreams . create_engine ( title = \"my-stream-engine\" ) @stream_engine . on_stop async def close_db () -> None : print ( \"Closing Database Connections\" ) await db_close () Source code in kstreams/engine.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def on_stop ( self , func : typing . Callable [[], typing . Any ], ) -> typing . Callable [[], typing . Any ]: \"\"\" A list of callables to run before the engine stops. Handler are callables that do not take any arguments, and may be either standard functions, or async functions. Attributes: func typing.Callable[[], typing.Any]: Func to callable before engine stops !!! Example ```python title=\"Engine before stops\" import kstreams stream_engine = kstreams.create_engine( title=\"my-stream-engine\" ) @stream_engine.on_stop async def close_db() -> None: print(\"Closing Database Connections\") await db_close() ``` \"\"\" self . _on_stop . append ( func ) return func","title":"on_stop()"},{"location":"engine/#kstreams.engine.StreamEngine.after_startup","text":"A list of callables to run after the engine starts. Handler are callables that do not take any arguments, and may be either standard functions, or async functions. Attributes: Name Type Description func Callable [[], Any ] Func to callable after engine starts Example Engine after startup import kstreams stream_engine = kstreams . create_engine ( title = \"my-stream-engine\" ) @stream_engine . after_startup async def after_startup () -> None : print ( \"Set pod as healthy\" ) await mark_healthy_pod () Source code in kstreams/engine.py 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 def after_startup ( self , func : typing . Callable [[], typing . Any ], ) -> typing . Callable [[], typing . Any ]: \"\"\" A list of callables to run after the engine starts. Handler are callables that do not take any arguments, and may be either standard functions, or async functions. Attributes: func typing.Callable[[], typing.Any]: Func to callable after engine starts !!! Example ```python title=\"Engine after startup\" import kstreams stream_engine = kstreams.create_engine( title=\"my-stream-engine\" ) @stream_engine.after_startup async def after_startup() -> None: print(\"Set pod as healthy\") await mark_healthy_pod() ``` \"\"\" self . _after_startup . append ( func ) return func","title":"after_startup()"},{"location":"engine/#kstreams.engine.StreamEngine.after_stop","text":"A list of callables to run after the engine stops. Handler are callables that do not take any arguments, and may be either standard functions, or async functions. Attributes: Name Type Description func Callable [[], Any ] Func to callable after engine stops Example Engine after stops import kstreams stream_engine = kstreams . create_engine ( title = \"my-stream-engine\" ) @stream_engine . after_stop async def after_stop () -> None : print ( \"Finishing backgrpund tasks\" ) Source code in kstreams/engine.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 def after_stop ( self , func : typing . Callable [[], typing . Any ], ) -> typing . Callable [[], typing . Any ]: \"\"\" A list of callables to run after the engine stops. Handler are callables that do not take any arguments, and may be either standard functions, or async functions. Attributes: func typing.Callable[[], typing.Any]: Func to callable after engine stops !!! Example ```python title=\"Engine after stops\" import kstreams stream_engine = kstreams.create_engine( title=\"my-stream-engine\" ) @stream_engine.after_stop async def after_stop() -> None: print(\"Finishing backgrpund tasks\") ``` \"\"\" self . _after_stop . append ( func ) return func","title":"after_stop()"},{"location":"getting_started/","text":"Getting Started You can starting using kstreams with simple producers and consumers and/or integrated it with any async framework like FastAPI Simple consumer and producer Simple use case import asyncio from kstreams import create_engine , ConsumerRecord stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ( \"local--py-stream\" , group_id = \"de-my-partition\" ) async def consume ( cr : ConsumerRecord ): print ( f \"Event consumed: headers: { cr . headers } , payload: { value } \" ) async def produce (): payload = b '{\"message\": \"Hello world!\"}' for i in range ( 5 ): metadata = await stream_engine . send ( \"local--py-streams\" , value = payload , key = \"1\" ) print ( f \"Message sent: { metadata } \" ) await asyncio . sleep ( 5 ) async def start (): await stream_engine . start () await produce () async def shutdown (): await stream_engine . stop () if __name__ == \"__main__\" : loop = asyncio . get_event_loop () try : loop . run_until_complete ( start ()) loop . run_forever () finally : loop . run_until_complete ( shutdown ()) loop . close () (This script is complete, it should run \"as is\") Recommended usage In the previous example you can see some boiler plate regarding how to start the program. We recommend to use aiorun , so you want have to worry about set signal handlers , shutdown callbacks , graceful shutdown and close the event loop . Usage with aiorun import aiorun from kstreams import create_engine , ConsumerRecord stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ( \"local--py-stream\" , group_id = \"de-my-partition\" ) async def consume ( cr : ConsumerRecord ): print ( f \"Event consumed: headers: { cr . headers } , payload: { value } \" ) async def produce (): payload = b '{\"message\": \"Hello world!\"}' for i in range ( 5 ): metadata = await stream_engine . send ( \"local--py-streams\" , value = payload , key = \"1\" ) print ( f \"Message sent: { metadata } \" ) await asyncio . sleep ( 5 ) async def start (): await stream_engine . start () await produce () async def shutdown ( loop ): await stream_engine . stop () if __name__ == \"__main__\" : aiorun . run ( start (), stop_on_unhandled_errors = True , shutdown_callback = shutdown ) (This script is complete, it should run \"as is\") FastAPI The following code example shows how kstreams can be integrated with any async framework like FastAPI . The full example can be found here First, we need to create an engine : Create the StreamEngine # streaming.engine.py from kstreams import create_engine stream_engine = create_engine ( title = \"my-stream-engine\" , ) Define the streams : Application stream # streaming.streams.py from .engine import stream_engine from kstreams import ConsumerRecord @stream_engine . stream ( \"local--kstream\" ) async def stream ( cr : ConsumerRecord ): print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . payload } \" ) Create the FastAPI : FastAPI # app.py from fastapi import FastAPI from starlette.responses import Response from starlette_prometheus import PrometheusMiddleware , metrics from .streaming.streams import stream_engine app = FastAPI () @app . on_event ( \"startup\" ) async def startup_event (): await stream_engine . start () @app . on_event ( \"shutdown\" ) async def shutdown_event (): await stream_engine . stop () @app . get ( \"/events\" ) async def post_produce_event () -> Response : payload = '{\"message\": \"hello world!\"}' metadata = await stream_engine . send ( \"local--kstream\" , value = payload . encode (), ) msg = ( f \"Produced event on topic: { metadata . topic } , \" f \"part: { metadata . partition } , offset: { metadata . offset } \" ) return Response ( msg ) app . add_middleware ( PrometheusMiddleware , filter_unhandled_paths = True ) app . add_api_route ( \"/metrics\" , metrics ) Changing Kafka settings To modify the settings of a cluster, like the servers, refer to the backends docs","title":"Getting Started"},{"location":"getting_started/#getting-started","text":"You can starting using kstreams with simple producers and consumers and/or integrated it with any async framework like FastAPI","title":"Getting Started"},{"location":"getting_started/#simple-consumer-and-producer","text":"Simple use case import asyncio from kstreams import create_engine , ConsumerRecord stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ( \"local--py-stream\" , group_id = \"de-my-partition\" ) async def consume ( cr : ConsumerRecord ): print ( f \"Event consumed: headers: { cr . headers } , payload: { value } \" ) async def produce (): payload = b '{\"message\": \"Hello world!\"}' for i in range ( 5 ): metadata = await stream_engine . send ( \"local--py-streams\" , value = payload , key = \"1\" ) print ( f \"Message sent: { metadata } \" ) await asyncio . sleep ( 5 ) async def start (): await stream_engine . start () await produce () async def shutdown (): await stream_engine . stop () if __name__ == \"__main__\" : loop = asyncio . get_event_loop () try : loop . run_until_complete ( start ()) loop . run_forever () finally : loop . run_until_complete ( shutdown ()) loop . close () (This script is complete, it should run \"as is\")","title":"Simple consumer and producer"},{"location":"getting_started/#recommended-usage","text":"In the previous example you can see some boiler plate regarding how to start the program. We recommend to use aiorun , so you want have to worry about set signal handlers , shutdown callbacks , graceful shutdown and close the event loop . Usage with aiorun import aiorun from kstreams import create_engine , ConsumerRecord stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ( \"local--py-stream\" , group_id = \"de-my-partition\" ) async def consume ( cr : ConsumerRecord ): print ( f \"Event consumed: headers: { cr . headers } , payload: { value } \" ) async def produce (): payload = b '{\"message\": \"Hello world!\"}' for i in range ( 5 ): metadata = await stream_engine . send ( \"local--py-streams\" , value = payload , key = \"1\" ) print ( f \"Message sent: { metadata } \" ) await asyncio . sleep ( 5 ) async def start (): await stream_engine . start () await produce () async def shutdown ( loop ): await stream_engine . stop () if __name__ == \"__main__\" : aiorun . run ( start (), stop_on_unhandled_errors = True , shutdown_callback = shutdown ) (This script is complete, it should run \"as is\")","title":"Recommended usage"},{"location":"getting_started/#fastapi","text":"The following code example shows how kstreams can be integrated with any async framework like FastAPI . The full example can be found here First, we need to create an engine : Create the StreamEngine # streaming.engine.py from kstreams import create_engine stream_engine = create_engine ( title = \"my-stream-engine\" , ) Define the streams : Application stream # streaming.streams.py from .engine import stream_engine from kstreams import ConsumerRecord @stream_engine . stream ( \"local--kstream\" ) async def stream ( cr : ConsumerRecord ): print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . payload } \" ) Create the FastAPI : FastAPI # app.py from fastapi import FastAPI from starlette.responses import Response from starlette_prometheus import PrometheusMiddleware , metrics from .streaming.streams import stream_engine app = FastAPI () @app . on_event ( \"startup\" ) async def startup_event (): await stream_engine . start () @app . on_event ( \"shutdown\" ) async def shutdown_event (): await stream_engine . stop () @app . get ( \"/events\" ) async def post_produce_event () -> Response : payload = '{\"message\": \"hello world!\"}' metadata = await stream_engine . send ( \"local--kstream\" , value = payload . encode (), ) msg = ( f \"Produced event on topic: { metadata . topic } , \" f \"part: { metadata . partition } , offset: { metadata . offset } \" ) return Response ( msg ) app . add_middleware ( PrometheusMiddleware , filter_unhandled_paths = True ) app . add_api_route ( \"/metrics\" , metrics )","title":"FastAPI"},{"location":"getting_started/#changing-kafka-settings","text":"To modify the settings of a cluster, like the servers, refer to the backends docs","title":"Changing Kafka settings"},{"location":"metrics/","text":"Metrics are generated by prometheus_client . You must be responsable of setting up a webserver to expose the metrics . Metrics Producer topic_partition_offsets : Gauge of offsets per topic/partition Consumer consumer_committed : Gauge of consumer commited per topic/partition in a consumer group consumer_position : Gauge of consumer current position per topic/partition in a consumer group consumer_highwater : Gauge of consumer highwater per topic/partition in a consumer group consumer_lag : Gauge of current consumer lag per topic/partition in a consumer group calculated with the last commited offset position_lag : Gauge of current consumer position_lag per topic/partition in a consumer group calculated using the consumer position","title":"Metrics"},{"location":"metrics/#metrics","text":"","title":"Metrics"},{"location":"metrics/#producer","text":"topic_partition_offsets : Gauge of offsets per topic/partition","title":"Producer"},{"location":"metrics/#consumer","text":"consumer_committed : Gauge of consumer commited per topic/partition in a consumer group consumer_position : Gauge of consumer current position per topic/partition in a consumer group consumer_highwater : Gauge of consumer highwater per topic/partition in a consumer group consumer_lag : Gauge of current consumer lag per topic/partition in a consumer group calculated with the last commited offset position_lag : Gauge of current consumer position_lag per topic/partition in a consumer group calculated using the consumer position","title":"Consumer"},{"location":"middleware/","text":"Middleware Kstreams allows you to include middlewares for adding behavior to streams. A middleware is a callable that works with every ConsumerRecord (CR) before and after it is processed by a specific stream . Middlewares also have access to the stream and send function. It takes each CR that arrives to a kafka topic . Then it can do something to the CR or run any needed code. Then it passes the CR to be processed by another callable (other middleware or stream). Once the CR is processed by the stream, the chain is \"completed\". If there is code after the self.next_call(cr) then it will be executed. Kstreams Middleware have the following protocol: Bases: Protocol Source code in kstreams/middleware/middleware.py 17 18 19 20 21 22 23 24 25 26 27 28 29 class MiddlewareProtocol ( typing . Protocol ): def __init__ ( self , * , next_call : types . NextMiddlewareCall , send : types . Send , stream : \"Stream\" , ** kwargs : typing . Any , ) -> None : ... # pragma: no cover async def __call__ ( self , cr : ConsumerRecord ) -> typing . Any : ... # pragma: no cover Note The __call__ method can return anything so previous calls can use the returned value. Make sure that the line return await self.next_call(cr) is in your method Warning Middlewares only work with the new Dependency Injection approach Creating a middleware To create a middleware you have to create a class that inherits from BaseMiddleware . Then, the method async def __call__ must be defined. Let's consider that we want to save the CR to elastic before it is processed: import typing from kstreams import ConsumerRecord , middleware async def save_to_elastic ( cr : ConsumerRecord ) -> None : ... class ElasticMiddleware ( middleware . BaseMiddleware ): async def __call__ ( self , cr : ConsumerRecord ) -> typing . Any : # save to elastic before calling the next await save_to_elastic ( cr ) # the next call could be another middleware return await self . next_call ( cr ) Then, we have to include the middleware: from kstreams import ConsumerRecord , middleware from .engine import stream_engine middlewares = [ middleware . Middleware ( ElasticMiddleware )] @stream_engine . stream ( \"kstreams-topic\" , middlewares = middlewares ) async def processor ( cr : ConsumerRecord ): ... Note The Middleware concept also applies for async generators (yield from a stream) Adding extra configuration to middlewares If you want to provide extra configuration to middleware you should override the init method with the extra options as keywargs and then call super().__init__(**kwargs) Let's consider that we want to send an event to a spcific topic when a ValueError is raised inside a stream (Dead Letter Queue) from kstreams import ConsumerRecord , types , Stream , middleware class DLQMiddleware ( middleware . BaseMiddleware ): def __init__ ( self , * , topic : str , ** kwargs ) -> None : super () . __init__ ( ** kwargs ) self . topic = topic async def __call__ ( self , cr : ConsumerRecord ): try : return await self . next_call ( cr ) except ValueError : await self . send ( self . topic , key = cr . key , value = cr . value ) # Create the middlewares middlewares = [ middleware . Middleware ( DLQMiddleware , topic = \"kstreams-dlq-topic\" ) ] @stream_engine . stream ( \"kstreams-topic\" , middlewares = middlewares ) async def processor ( cr : ConsumerRecord ): if cr . value == b \"joker\" : raise ValueError ( \"Joker received...\" ) Default Middleware Source code in kstreams/middleware/middleware.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 class ExceptionMiddleware ( BaseMiddleware ): def __init__ ( self , * , engine : \"StreamEngine\" , error_policy : StreamErrorPolicy , ** kwargs ) -> None : super () . __init__ ( ** kwargs ) self . engine = engine self . error_policy = error_policy async def __call__ ( self , cr : ConsumerRecord ) -> typing . Any : try : return await self . next_call ( cr ) except errors . ConsumerStoppedError as exc : await self . cleanup_policy ( exc ) except Exception as exc : logger . exception ( \"Unhandled error occurred while listening to the stream. \" f \"Stream consuming from topics { self . stream . topics } CRASHED!!! \\n\\n \" ) if sys . version_info >= ( 3 , 11 ): exc . add_note ( f \"Handler: { self . stream . func } \" ) exc . add_note ( f \"Topics: { self . stream . topics } \" ) await self . cleanup_policy ( exc ) async def cleanup_policy ( self , exc : Exception ) -> None : # always release the asyncio.Lock `is_processing` to # stop or restart properly the `stream` self . stream . is_processing . release () if self . error_policy == StreamErrorPolicy . RESTART : await self . stream . stop () logger . info ( f \"Restarting stream { self . stream } \" ) await self . stream . start () elif self . error_policy == StreamErrorPolicy . STOP : await self . stream . stop () raise exc else : await self . engine . stop () raise exc # acquire the asyncio.Lock `is_processing` again to resume the processing # and avoid `RuntimeError: Lock is not acquired.` await self . stream . is_processing . acquire () Middleware chain It is possible to add as many middlewares as you want to split and reuse business logic, however the downside is extra complexity and the code might become slower. The middleware order is important as they are evaluated in the order that were placed in the stream. In the following example we are adding three middelwares in the following order: DLQMiddleware , ElasticMiddleware , and S3Middleware . The code chain execution will be: sequenceDiagram autonumber ExceptionMiddleware->>DLQMiddleware: Note left of ExceptionMiddleware: Event received alt No Processing Error DLQMiddleware->>ElasticMiddleware: Note right of ElasticMiddleware: Store CR on Elastic ElasticMiddleware->>S3Middleware: Note right of S3Middleware: Store CR on S3 S3Middleware->>Stream: Note right of Stream: CR processed Stream-->>S3Middleware: S3Middleware-->>ElasticMiddleware: ElasticMiddleware-->>DLQMiddleware: DLQMiddleware-->>ExceptionMiddleware: end Multiple middlewares example from kstreams import ConsumerRecord , Stream , middleware class DLQMiddleware ( middleware . BaseMiddleware ): async def __call__ ( self , cr : ConsumerRecord ): try : return await self . next_call ( cr ) except ValueError : await dlq ( cr . value ) class ElasticMiddleware ( middleware . BaseMiddleware ): async def __call__ ( self , cr : ConsumerRecord ): await save_to_elastic ( cr . value ) return await self . next_call ( cr ) class S3Middleware ( middleware . BaseMiddleware ): async def __call__ ( self , cr : ConsumerRecord ): await backup_to_s3 ( cr . value ) return await self . next_call ( cr ) middlewares = [ middleware . Middleware ( DLQMiddleware ), middleware . Middleware ( ElasticMiddleware ), middleware . Middleware ( S3Middleware ), ] @stream_engine . stream ( \"kstreams-topic\" , middlewares = middlewares ) async def processor ( cr : ConsumerRecord ): if cr . value == event_2 : raise ValueError ( \"Error from stream...\" ) await save_to_db ( cr . value ) Note In the example we can see that always the cr will be save into elastic and s3 regardless an error Executing Code after the CR was processed As mentioned in the introduction, it is possible to execute code after the CR is handled. To do this, we need to place code after next_call is called: Execute code after CR is handled from kstreams import ConsumerRecord , Stream , middleware class DLQMiddleware ( middleware . BaseMiddleware ): async def __call__ ( self , cr : ConsumerRecord ): try : return await self . next_call ( cr ) except ValueError : await dlq ( cr . value ) class ElasticMiddleware ( middleware . BaseMiddleware ): async def __call__ ( self , cr : ConsumerRecord ): return await self . next_call ( cr ) # This will be called after the whole chain has finished await save_to_elastic ( cr . value ) middlewares = [ middleware . Middleware ( DLQMiddleware ), middleware . Middleware ( ElasticMiddleware ), ] @stream_engine . stream ( \"kstreams-topic\" , middlewares = middlewares ) async def processor ( cr : ConsumerRecord ): if cr . value == event_2 : raise ValueError ( \"Error from stream...\" ) await save_to_db ( cr . value ) Note In the example we can see that only if there is not an error the event is saved to elastic Deserialization To deserialize bytes into a different structure like dict middlewares are the preferred way to it. Examples: Source code in examples/dataclasses-avroschema-example/dataclasses_avroschema_example/middlewares.py 6 7 8 9 10 11 12 13 14 15 16 17 18 class AvroDeserializerMiddleware ( middleware . BaseMiddleware ): def __init__ ( self , * , model : AvroModel , ** kwargs ) -> None : super () . __init__ ( ** kwargs ) self . model = model async def __call__ ( self , cr : ConsumerRecord ): \"\"\" Deserialize a payload to an AvroModel \"\"\" if cr . value is not None : data = self . model . deserialize ( cr . value ) cr . value = data return await self . next_call ( cr ) Source code in examples/confluent-example/confluent_example/middlewares.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class ConfluentMiddlewareDeserializer ( middleware . BaseMiddleware , AsyncAvroMessageSerializer ): def __init__ ( self , * , schema_registry_client : AsyncSchemaRegistryClient , reader_schema : Optional [ schema . AvroSchema ] = None , return_record_name : bool = False , ** kwargs , ): super () . __init__ ( ** kwargs ) self . schemaregistry_client = schema_registry_client self . reader_schema = reader_schema self . return_record_name = return_record_name self . id_to_decoder_func : Dict = {} self . id_to_writers : Dict = {} async def __call__ ( self , cr : ConsumerRecord ): \"\"\" Deserialize the event to a dict \"\"\" data = await self . decode_message ( cr . value ) cr . value = data return await self . next_call ( cr )","title":"Middleware"},{"location":"middleware/#middleware","text":"Kstreams allows you to include middlewares for adding behavior to streams. A middleware is a callable that works with every ConsumerRecord (CR) before and after it is processed by a specific stream . Middlewares also have access to the stream and send function. It takes each CR that arrives to a kafka topic . Then it can do something to the CR or run any needed code. Then it passes the CR to be processed by another callable (other middleware or stream). Once the CR is processed by the stream, the chain is \"completed\". If there is code after the self.next_call(cr) then it will be executed. Kstreams Middleware have the following protocol: Bases: Protocol Source code in kstreams/middleware/middleware.py 17 18 19 20 21 22 23 24 25 26 27 28 29 class MiddlewareProtocol ( typing . Protocol ): def __init__ ( self , * , next_call : types . NextMiddlewareCall , send : types . Send , stream : \"Stream\" , ** kwargs : typing . Any , ) -> None : ... # pragma: no cover async def __call__ ( self , cr : ConsumerRecord ) -> typing . Any : ... # pragma: no cover Note The __call__ method can return anything so previous calls can use the returned value. Make sure that the line return await self.next_call(cr) is in your method Warning Middlewares only work with the new Dependency Injection approach","title":"Middleware"},{"location":"middleware/#creating-a-middleware","text":"To create a middleware you have to create a class that inherits from BaseMiddleware . Then, the method async def __call__ must be defined. Let's consider that we want to save the CR to elastic before it is processed: import typing from kstreams import ConsumerRecord , middleware async def save_to_elastic ( cr : ConsumerRecord ) -> None : ... class ElasticMiddleware ( middleware . BaseMiddleware ): async def __call__ ( self , cr : ConsumerRecord ) -> typing . Any : # save to elastic before calling the next await save_to_elastic ( cr ) # the next call could be another middleware return await self . next_call ( cr ) Then, we have to include the middleware: from kstreams import ConsumerRecord , middleware from .engine import stream_engine middlewares = [ middleware . Middleware ( ElasticMiddleware )] @stream_engine . stream ( \"kstreams-topic\" , middlewares = middlewares ) async def processor ( cr : ConsumerRecord ): ... Note The Middleware concept also applies for async generators (yield from a stream)","title":"Creating a middleware"},{"location":"middleware/#adding-extra-configuration-to-middlewares","text":"If you want to provide extra configuration to middleware you should override the init method with the extra options as keywargs and then call super().__init__(**kwargs) Let's consider that we want to send an event to a spcific topic when a ValueError is raised inside a stream (Dead Letter Queue) from kstreams import ConsumerRecord , types , Stream , middleware class DLQMiddleware ( middleware . BaseMiddleware ): def __init__ ( self , * , topic : str , ** kwargs ) -> None : super () . __init__ ( ** kwargs ) self . topic = topic async def __call__ ( self , cr : ConsumerRecord ): try : return await self . next_call ( cr ) except ValueError : await self . send ( self . topic , key = cr . key , value = cr . value ) # Create the middlewares middlewares = [ middleware . Middleware ( DLQMiddleware , topic = \"kstreams-dlq-topic\" ) ] @stream_engine . stream ( \"kstreams-topic\" , middlewares = middlewares ) async def processor ( cr : ConsumerRecord ): if cr . value == b \"joker\" : raise ValueError ( \"Joker received...\" )","title":"Adding extra configuration to middlewares"},{"location":"middleware/#default-middleware","text":"Source code in kstreams/middleware/middleware.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 class ExceptionMiddleware ( BaseMiddleware ): def __init__ ( self , * , engine : \"StreamEngine\" , error_policy : StreamErrorPolicy , ** kwargs ) -> None : super () . __init__ ( ** kwargs ) self . engine = engine self . error_policy = error_policy async def __call__ ( self , cr : ConsumerRecord ) -> typing . Any : try : return await self . next_call ( cr ) except errors . ConsumerStoppedError as exc : await self . cleanup_policy ( exc ) except Exception as exc : logger . exception ( \"Unhandled error occurred while listening to the stream. \" f \"Stream consuming from topics { self . stream . topics } CRASHED!!! \\n\\n \" ) if sys . version_info >= ( 3 , 11 ): exc . add_note ( f \"Handler: { self . stream . func } \" ) exc . add_note ( f \"Topics: { self . stream . topics } \" ) await self . cleanup_policy ( exc ) async def cleanup_policy ( self , exc : Exception ) -> None : # always release the asyncio.Lock `is_processing` to # stop or restart properly the `stream` self . stream . is_processing . release () if self . error_policy == StreamErrorPolicy . RESTART : await self . stream . stop () logger . info ( f \"Restarting stream { self . stream } \" ) await self . stream . start () elif self . error_policy == StreamErrorPolicy . STOP : await self . stream . stop () raise exc else : await self . engine . stop () raise exc # acquire the asyncio.Lock `is_processing` again to resume the processing # and avoid `RuntimeError: Lock is not acquired.` await self . stream . is_processing . acquire ()","title":"Default Middleware"},{"location":"middleware/#middleware-chain","text":"It is possible to add as many middlewares as you want to split and reuse business logic, however the downside is extra complexity and the code might become slower. The middleware order is important as they are evaluated in the order that were placed in the stream. In the following example we are adding three middelwares in the following order: DLQMiddleware , ElasticMiddleware , and S3Middleware . The code chain execution will be: sequenceDiagram autonumber ExceptionMiddleware->>DLQMiddleware: Note left of ExceptionMiddleware: Event received alt No Processing Error DLQMiddleware->>ElasticMiddleware: Note right of ElasticMiddleware: Store CR on Elastic ElasticMiddleware->>S3Middleware: Note right of S3Middleware: Store CR on S3 S3Middleware->>Stream: Note right of Stream: CR processed Stream-->>S3Middleware: S3Middleware-->>ElasticMiddleware: ElasticMiddleware-->>DLQMiddleware: DLQMiddleware-->>ExceptionMiddleware: end Multiple middlewares example from kstreams import ConsumerRecord , Stream , middleware class DLQMiddleware ( middleware . BaseMiddleware ): async def __call__ ( self , cr : ConsumerRecord ): try : return await self . next_call ( cr ) except ValueError : await dlq ( cr . value ) class ElasticMiddleware ( middleware . BaseMiddleware ): async def __call__ ( self , cr : ConsumerRecord ): await save_to_elastic ( cr . value ) return await self . next_call ( cr ) class S3Middleware ( middleware . BaseMiddleware ): async def __call__ ( self , cr : ConsumerRecord ): await backup_to_s3 ( cr . value ) return await self . next_call ( cr ) middlewares = [ middleware . Middleware ( DLQMiddleware ), middleware . Middleware ( ElasticMiddleware ), middleware . Middleware ( S3Middleware ), ] @stream_engine . stream ( \"kstreams-topic\" , middlewares = middlewares ) async def processor ( cr : ConsumerRecord ): if cr . value == event_2 : raise ValueError ( \"Error from stream...\" ) await save_to_db ( cr . value ) Note In the example we can see that always the cr will be save into elastic and s3 regardless an error","title":"Middleware chain"},{"location":"middleware/#executing-code-after-the-cr-was-processed","text":"As mentioned in the introduction, it is possible to execute code after the CR is handled. To do this, we need to place code after next_call is called: Execute code after CR is handled from kstreams import ConsumerRecord , Stream , middleware class DLQMiddleware ( middleware . BaseMiddleware ): async def __call__ ( self , cr : ConsumerRecord ): try : return await self . next_call ( cr ) except ValueError : await dlq ( cr . value ) class ElasticMiddleware ( middleware . BaseMiddleware ): async def __call__ ( self , cr : ConsumerRecord ): return await self . next_call ( cr ) # This will be called after the whole chain has finished await save_to_elastic ( cr . value ) middlewares = [ middleware . Middleware ( DLQMiddleware ), middleware . Middleware ( ElasticMiddleware ), ] @stream_engine . stream ( \"kstreams-topic\" , middlewares = middlewares ) async def processor ( cr : ConsumerRecord ): if cr . value == event_2 : raise ValueError ( \"Error from stream...\" ) await save_to_db ( cr . value ) Note In the example we can see that only if there is not an error the event is saved to elastic","title":"Executing Code after the CR was processed"},{"location":"middleware/#deserialization","text":"To deserialize bytes into a different structure like dict middlewares are the preferred way to it. Examples: Source code in examples/dataclasses-avroschema-example/dataclasses_avroschema_example/middlewares.py 6 7 8 9 10 11 12 13 14 15 16 17 18 class AvroDeserializerMiddleware ( middleware . BaseMiddleware ): def __init__ ( self , * , model : AvroModel , ** kwargs ) -> None : super () . __init__ ( ** kwargs ) self . model = model async def __call__ ( self , cr : ConsumerRecord ): \"\"\" Deserialize a payload to an AvroModel \"\"\" if cr . value is not None : data = self . model . deserialize ( cr . value ) cr . value = data return await self . next_call ( cr ) Source code in examples/confluent-example/confluent_example/middlewares.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class ConfluentMiddlewareDeserializer ( middleware . BaseMiddleware , AsyncAvroMessageSerializer ): def __init__ ( self , * , schema_registry_client : AsyncSchemaRegistryClient , reader_schema : Optional [ schema . AvroSchema ] = None , return_record_name : bool = False , ** kwargs , ): super () . __init__ ( ** kwargs ) self . schemaregistry_client = schema_registry_client self . reader_schema = reader_schema self . return_record_name = return_record_name self . id_to_decoder_func : Dict = {} self . id_to_writers : Dict = {} async def __call__ ( self , cr : ConsumerRecord ): \"\"\" Deserialize the event to a dict \"\"\" data = await self . decode_message ( cr . value ) cr . value = data return await self . next_call ( cr )","title":"Deserialization"},{"location":"monitoring/","text":"This page discusses how to monitor your application using the Kafka metrics that are accessible in Prometheus. Before we begin, it's crucial to note that Kafka itself makes a number of useful metrics available, including the cluster, broker, and clients (producer and consumers). This means that we can quickly add some graphs to our dashboards by utilizing the already-exposed metrics. Kstreams includes a collection of metrics. See Metrics Docs for more information. kstreams.PrometheusMonitor Metrics monitor to keep track of Producers and Consumers. Attributes: metrics_scrape_time float: Amount of seconds that the monitor will wait until next scrape iteration Source code in kstreams/prometheus/monitor.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 class PrometheusMonitor : \"\"\" Metrics monitor to keep track of Producers and Consumers. Attributes: metrics_scrape_time float: Amount of seconds that the monitor will wait until next scrape iteration \"\"\" # Producer metrics MET_OFFSETS = Gauge ( \"topic_partition_offsets\" , \"help producer offsets\" , [ \"topic\" , \"partition\" ] ) # Consumer metrics MET_COMMITTED = Gauge ( \"consumer_committed\" , \"help consumer committed\" , [ \"topic\" , \"partition\" , \"consumer_group\" ], ) MET_POSITION = Gauge ( \"consumer_position\" , \"help consumer position\" , [ \"topic\" , \"partition\" , \"consumer_group\" ], ) MET_HIGHWATER = Gauge ( \"consumer_highwater\" , \"help consumer highwater\" , [ \"topic\" , \"partition\" , \"consumer_group\" ], ) MET_LAG = Gauge ( \"consumer_lag\" , \"help consumer lag calculated using the last commited offset\" , [ \"topic\" , \"partition\" , \"consumer_group\" ], ) MET_POSITION_LAG = Gauge ( \"position_lag\" , \"help consumer position lag calculated using the consumer position\" , [ \"topic\" , \"partition\" , \"consumer_group\" ], ) def __init__ ( self , metrics_scrape_time : float = 3 ): self . metrics_scrape_time = metrics_scrape_time self . running = False self . _producer = None self . _streams : List [ Stream ] = [] async def start ( self ) -> None : self . running = True logger . info ( \"Starting Prometheus Monitoring started...\" ) await self . _metrics_task () async def stop ( self ) -> None : self . running = False self . _clean_consumer_metrics () logger . info ( \"Prometheus Monitoring stopped...\" ) def add_topic_partition_offset ( self , topic : str , partition : int , offset : int ) -> None : self . MET_OFFSETS . labels ( topic = topic , partition = partition ) . set ( offset ) def _add_consumer_metrics ( self , metrics_dict : MetricsType ): for topic_partition , partitions_metadata in metrics_dict . items (): group_id = partitions_metadata [ \"group_id\" ] position = partitions_metadata [ \"position\" ] committed = partitions_metadata [ \"committed\" ] highwater = partitions_metadata [ \"highwater\" ] lag = partitions_metadata [ \"lag\" ] position_lag = partitions_metadata [ \"position_lag\" ] self . MET_COMMITTED . labels ( topic = topic_partition . topic , partition = topic_partition . partition , consumer_group = group_id , ) . set ( committed or 0 ) self . MET_POSITION . labels ( topic = topic_partition . topic , partition = topic_partition . partition , consumer_group = group_id , ) . set ( position or - 1 ) self . MET_HIGHWATER . labels ( topic = topic_partition . topic , partition = topic_partition . partition , consumer_group = group_id , ) . set ( highwater or 0 ) self . MET_LAG . labels ( topic = topic_partition . topic , partition = topic_partition . partition , consumer_group = group_id , ) . set ( lag or 0 ) self . MET_POSITION_LAG . labels ( topic = topic_partition . topic , partition = topic_partition . partition , consumer_group = group_id , ) . set ( position_lag or 0 ) def _clean_consumer_metrics ( self ) -> None : \"\"\" This method should be called when a rebalance takes place to clean all consumers metrics. When the rebalance finishes new metrics will be generated per consumer based on the consumer assigments \"\"\" self . MET_LAG . clear () self . MET_POSITION_LAG . clear () self . MET_COMMITTED . clear () self . MET_POSITION . clear () self . MET_HIGHWATER . clear () def clean_stream_consumer_metrics ( self , consumer : Consumer ) -> None : topic_partitions = consumer . assignment () group_id = consumer . _group_id for topic_partition in topic_partitions : topic = topic_partition . topic partition = topic_partition . partition metrics_found = False for sample in self . MET_LAG . collect ()[ 0 ] . samples : if { \"topic\" : topic , \"partition\" : str ( partition ), \"consumer_group\" : group_id , } == sample . labels : metrics_found = True if metrics_found : self . MET_LAG . remove ( topic , partition , group_id ) self . MET_POSITION_LAG . remove ( topic , partition , group_id ) self . MET_COMMITTED . remove ( topic , partition , group_id ) self . MET_POSITION . remove ( topic , partition , group_id ) self . MET_HIGHWATER . remove ( topic , partition , group_id ) else : logger . debug ( \"Metrics for consumer with group-id: \" f \" { consumer . _group_id } not found\" ) def add_producer ( self , producer ): self . _producer = producer def add_streams ( self , streams ): self . _streams = streams async def generate_consumer_metrics ( self , consumer : Consumer ): \"\"\" Generate Consumer Metrics for Prometheus Format: { \"topic-1\": { \"1\": ( [topic-1, partition-number, 'group-id-1'], committed, position, highwater, lag, position_lag ) \"2\": ( [topic-1, partition-number, 'group-id-1'], committed, position, highwater, lag, position_lag ) }, ... \"topic-n\": { \"1\": ( [topic-n, partition-number, 'group-id-n'], committed, position, highwater, lag, position_lag ) \"2\": ( [topic-n, partition-number, 'group-id-n'], committed, position, highwater, lag, position_lag ) } } \"\"\" metrics : MetricsType = DefaultDict ( dict ) topic_partitions = consumer . assignment () for topic_partition in topic_partitions : committed = await consumer . committed ( topic_partition ) or 0 position = await consumer . position ( topic_partition ) highwater = consumer . highwater ( topic_partition ) lag = position_lag = None if highwater : lag = highwater - committed position_lag = highwater - position metrics [ topic_partition ] = { \"group_id\" : consumer . _group_id , \"committed\" : committed , \"position\" : position , \"highwater\" : highwater , \"lag\" : lag , \"position_lag\" : position_lag , } self . _add_consumer_metrics ( metrics ) async def _metrics_task ( self ) -> None : \"\"\" Task that runs in `backgroud` to generate consumer metrics. When self.running is False the task will finish and it will be safe to stop consumers and producers. \"\"\" while self . running : await asyncio . sleep ( self . metrics_scrape_time ) for stream in self . _streams : if stream . consumer is not None : try : await self . generate_consumer_metrics ( stream . consumer ) except RuntimeError : logger . debug ( f \"Metrics for stream { stream . name } can not be generated \" \"probably because it has been removed\" ) generate_consumer_metrics ( consumer ) async Generate Consumer Metrics for Prometheus Format { \"topic-1\": { \"1\": ( [topic-1, partition-number, 'group-id-1'], committed, position, highwater, lag, position_lag ) \"2\": ( [topic-1, partition-number, 'group-id-1'], committed, position, highwater, lag, position_lag ) }, ... \"topic-n\": { \"1\": ( [topic-n, partition-number, 'group-id-n'], committed, position, highwater, lag, position_lag ) \"2\": ( [topic-n, partition-number, 'group-id-n'], committed, position, highwater, lag, position_lag ) } } Source code in kstreams/prometheus/monitor.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 async def generate_consumer_metrics ( self , consumer : Consumer ): \"\"\" Generate Consumer Metrics for Prometheus Format: { \"topic-1\": { \"1\": ( [topic-1, partition-number, 'group-id-1'], committed, position, highwater, lag, position_lag ) \"2\": ( [topic-1, partition-number, 'group-id-1'], committed, position, highwater, lag, position_lag ) }, ... \"topic-n\": { \"1\": ( [topic-n, partition-number, 'group-id-n'], committed, position, highwater, lag, position_lag ) \"2\": ( [topic-n, partition-number, 'group-id-n'], committed, position, highwater, lag, position_lag ) } } \"\"\" metrics : MetricsType = DefaultDict ( dict ) topic_partitions = consumer . assignment () for topic_partition in topic_partitions : committed = await consumer . committed ( topic_partition ) or 0 position = await consumer . position ( topic_partition ) highwater = consumer . highwater ( topic_partition ) lag = position_lag = None if highwater : lag = highwater - committed position_lag = highwater - position metrics [ topic_partition ] = { \"group_id\" : consumer . _group_id , \"committed\" : committed , \"position\" : position , \"highwater\" : highwater , \"lag\" : lag , \"position_lag\" : position_lag , } self . _add_consumer_metrics ( metrics ) Consumer Metrics We advise including the consumer_lag in your application's grafana dashboard. consumer_lag will show you how far your consumers are lagging behind the published events in the topic they are reading. For instance, if you have a single consumer and another team is producing millions of events, the consumer might not be able to handle them in time (where in time is defined by you, like: \"in an hour of receiving a message it should be consumed\"). Based on the lag, you will have to develop your own alerts. An alert should be pushed to Slack if you experience more than a particular amount of lag. You will require your consumer_group name in order to design a basic dashboard using the consumer_lag . We could add a query in Grafana like this: sum ( kafka_consumer_group_ConsumerLagMetrics_Value { topic =~ \" YOUR_OWN_TOPIC_NAME \", groupId =~ \" YOUR_CONSUMER_GROUP \", name = \" SumOffsetLag \"} ) by ( topic ) Remember to replace YOUR_CONSUMER_GROUP and YOUR_OWN_TOPIC_NAME with your consumer_group and topic respectively \u2b06\ufe0f Producer Metrics If you have producers, it's a good idea to monitor the growth of Log End Offset (LEO). The increase in LEO indicates the number of events produced in the last N minutes. If you know that events should occur every N minutes, you can trigger alerts if no events occur because this metric will tell you whether or not events occurred. We could add a query in Grafana like this, where N is 10m : sum ( max ( increase ( kafka_log_Log_Value { name = \" LogEndOffset \", topic =~ \" TOPIC_NAME \"}[ 10m ] )) by ( partition , topic )) by ( topic ) Remember to modify TOPIC_NAME to the name of the topic you want to track \u2b06\ufe0f Custom Business Metrics One benefit of Prometheus is that you can design your own custom metrics. Scenario : Consider an event-based ordering system. Assume you receive X orders daily and ship Y orders daily. Most likely, you will create a dashboard using this data. Fortunately, we can create our own custom metrics by using the Prometheus Python client. You can construct a variety of metrics with prometheus: Gauge Counter Histogram Summary You can read more about it in prometheus metric_types website. In our scenario, we will most likely want a Counter for orders received and a Counter for orders shipped. from prometheus_client import Counter from kstreams import PrometheusMonitor class MyAppPrometheusMonitor ( PrometheusMonitor ): def __init__ ( self ): super () . __init__ () # initialize kstream metrics self . orders_received = Counter ( 'orders_received' , 'Amount of orders received' ) self . orders_shipped = Counter ( 'orders_shipped' , 'Amount of orders shipped' ) def increase_received ( self , amount : int = 1 ): self . orders_received . inc ( amount ) def increase_shipped ( self , amount : int = 1 ): self . orders_shipped . inc ( amount ) In our kstreams app, we can: stream_engine = create_engine ( title = \"my-engine\" , monitor = MyAppPrometheusMonitor ()) @stream_engine . stream ( \"my-special-orders\" ) async def consume_orders_received ( cr : ConsumerRecord ): if cr . value . status == \"NEW\" : stream_engine . monitor . increase_received () elif cr . value . status == \"SHIPPED\" : stream_engine . monitor . increase_shipped () Your app's prometheus would display this data, which you might utilize to build a stylish \u2728dashboard\u2728 interface. For further details, see the Prometheus python client documentation.","title":"Monitoring"},{"location":"monitoring/#kstreams.PrometheusMonitor","text":"Metrics monitor to keep track of Producers and Consumers. Attributes: metrics_scrape_time float: Amount of seconds that the monitor will wait until next scrape iteration Source code in kstreams/prometheus/monitor.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 class PrometheusMonitor : \"\"\" Metrics monitor to keep track of Producers and Consumers. Attributes: metrics_scrape_time float: Amount of seconds that the monitor will wait until next scrape iteration \"\"\" # Producer metrics MET_OFFSETS = Gauge ( \"topic_partition_offsets\" , \"help producer offsets\" , [ \"topic\" , \"partition\" ] ) # Consumer metrics MET_COMMITTED = Gauge ( \"consumer_committed\" , \"help consumer committed\" , [ \"topic\" , \"partition\" , \"consumer_group\" ], ) MET_POSITION = Gauge ( \"consumer_position\" , \"help consumer position\" , [ \"topic\" , \"partition\" , \"consumer_group\" ], ) MET_HIGHWATER = Gauge ( \"consumer_highwater\" , \"help consumer highwater\" , [ \"topic\" , \"partition\" , \"consumer_group\" ], ) MET_LAG = Gauge ( \"consumer_lag\" , \"help consumer lag calculated using the last commited offset\" , [ \"topic\" , \"partition\" , \"consumer_group\" ], ) MET_POSITION_LAG = Gauge ( \"position_lag\" , \"help consumer position lag calculated using the consumer position\" , [ \"topic\" , \"partition\" , \"consumer_group\" ], ) def __init__ ( self , metrics_scrape_time : float = 3 ): self . metrics_scrape_time = metrics_scrape_time self . running = False self . _producer = None self . _streams : List [ Stream ] = [] async def start ( self ) -> None : self . running = True logger . info ( \"Starting Prometheus Monitoring started...\" ) await self . _metrics_task () async def stop ( self ) -> None : self . running = False self . _clean_consumer_metrics () logger . info ( \"Prometheus Monitoring stopped...\" ) def add_topic_partition_offset ( self , topic : str , partition : int , offset : int ) -> None : self . MET_OFFSETS . labels ( topic = topic , partition = partition ) . set ( offset ) def _add_consumer_metrics ( self , metrics_dict : MetricsType ): for topic_partition , partitions_metadata in metrics_dict . items (): group_id = partitions_metadata [ \"group_id\" ] position = partitions_metadata [ \"position\" ] committed = partitions_metadata [ \"committed\" ] highwater = partitions_metadata [ \"highwater\" ] lag = partitions_metadata [ \"lag\" ] position_lag = partitions_metadata [ \"position_lag\" ] self . MET_COMMITTED . labels ( topic = topic_partition . topic , partition = topic_partition . partition , consumer_group = group_id , ) . set ( committed or 0 ) self . MET_POSITION . labels ( topic = topic_partition . topic , partition = topic_partition . partition , consumer_group = group_id , ) . set ( position or - 1 ) self . MET_HIGHWATER . labels ( topic = topic_partition . topic , partition = topic_partition . partition , consumer_group = group_id , ) . set ( highwater or 0 ) self . MET_LAG . labels ( topic = topic_partition . topic , partition = topic_partition . partition , consumer_group = group_id , ) . set ( lag or 0 ) self . MET_POSITION_LAG . labels ( topic = topic_partition . topic , partition = topic_partition . partition , consumer_group = group_id , ) . set ( position_lag or 0 ) def _clean_consumer_metrics ( self ) -> None : \"\"\" This method should be called when a rebalance takes place to clean all consumers metrics. When the rebalance finishes new metrics will be generated per consumer based on the consumer assigments \"\"\" self . MET_LAG . clear () self . MET_POSITION_LAG . clear () self . MET_COMMITTED . clear () self . MET_POSITION . clear () self . MET_HIGHWATER . clear () def clean_stream_consumer_metrics ( self , consumer : Consumer ) -> None : topic_partitions = consumer . assignment () group_id = consumer . _group_id for topic_partition in topic_partitions : topic = topic_partition . topic partition = topic_partition . partition metrics_found = False for sample in self . MET_LAG . collect ()[ 0 ] . samples : if { \"topic\" : topic , \"partition\" : str ( partition ), \"consumer_group\" : group_id , } == sample . labels : metrics_found = True if metrics_found : self . MET_LAG . remove ( topic , partition , group_id ) self . MET_POSITION_LAG . remove ( topic , partition , group_id ) self . MET_COMMITTED . remove ( topic , partition , group_id ) self . MET_POSITION . remove ( topic , partition , group_id ) self . MET_HIGHWATER . remove ( topic , partition , group_id ) else : logger . debug ( \"Metrics for consumer with group-id: \" f \" { consumer . _group_id } not found\" ) def add_producer ( self , producer ): self . _producer = producer def add_streams ( self , streams ): self . _streams = streams async def generate_consumer_metrics ( self , consumer : Consumer ): \"\"\" Generate Consumer Metrics for Prometheus Format: { \"topic-1\": { \"1\": ( [topic-1, partition-number, 'group-id-1'], committed, position, highwater, lag, position_lag ) \"2\": ( [topic-1, partition-number, 'group-id-1'], committed, position, highwater, lag, position_lag ) }, ... \"topic-n\": { \"1\": ( [topic-n, partition-number, 'group-id-n'], committed, position, highwater, lag, position_lag ) \"2\": ( [topic-n, partition-number, 'group-id-n'], committed, position, highwater, lag, position_lag ) } } \"\"\" metrics : MetricsType = DefaultDict ( dict ) topic_partitions = consumer . assignment () for topic_partition in topic_partitions : committed = await consumer . committed ( topic_partition ) or 0 position = await consumer . position ( topic_partition ) highwater = consumer . highwater ( topic_partition ) lag = position_lag = None if highwater : lag = highwater - committed position_lag = highwater - position metrics [ topic_partition ] = { \"group_id\" : consumer . _group_id , \"committed\" : committed , \"position\" : position , \"highwater\" : highwater , \"lag\" : lag , \"position_lag\" : position_lag , } self . _add_consumer_metrics ( metrics ) async def _metrics_task ( self ) -> None : \"\"\" Task that runs in `backgroud` to generate consumer metrics. When self.running is False the task will finish and it will be safe to stop consumers and producers. \"\"\" while self . running : await asyncio . sleep ( self . metrics_scrape_time ) for stream in self . _streams : if stream . consumer is not None : try : await self . generate_consumer_metrics ( stream . consumer ) except RuntimeError : logger . debug ( f \"Metrics for stream { stream . name } can not be generated \" \"probably because it has been removed\" )","title":"PrometheusMonitor"},{"location":"monitoring/#kstreams.PrometheusMonitor.generate_consumer_metrics","text":"Generate Consumer Metrics for Prometheus Format { \"topic-1\": { \"1\": ( [topic-1, partition-number, 'group-id-1'], committed, position, highwater, lag, position_lag ) \"2\": ( [topic-1, partition-number, 'group-id-1'], committed, position, highwater, lag, position_lag ) }, ... \"topic-n\": { \"1\": ( [topic-n, partition-number, 'group-id-n'], committed, position, highwater, lag, position_lag ) \"2\": ( [topic-n, partition-number, 'group-id-n'], committed, position, highwater, lag, position_lag ) } } Source code in kstreams/prometheus/monitor.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 async def generate_consumer_metrics ( self , consumer : Consumer ): \"\"\" Generate Consumer Metrics for Prometheus Format: { \"topic-1\": { \"1\": ( [topic-1, partition-number, 'group-id-1'], committed, position, highwater, lag, position_lag ) \"2\": ( [topic-1, partition-number, 'group-id-1'], committed, position, highwater, lag, position_lag ) }, ... \"topic-n\": { \"1\": ( [topic-n, partition-number, 'group-id-n'], committed, position, highwater, lag, position_lag ) \"2\": ( [topic-n, partition-number, 'group-id-n'], committed, position, highwater, lag, position_lag ) } } \"\"\" metrics : MetricsType = DefaultDict ( dict ) topic_partitions = consumer . assignment () for topic_partition in topic_partitions : committed = await consumer . committed ( topic_partition ) or 0 position = await consumer . position ( topic_partition ) highwater = consumer . highwater ( topic_partition ) lag = position_lag = None if highwater : lag = highwater - committed position_lag = highwater - position metrics [ topic_partition ] = { \"group_id\" : consumer . _group_id , \"committed\" : committed , \"position\" : position , \"highwater\" : highwater , \"lag\" : lag , \"position_lag\" : position_lag , } self . _add_consumer_metrics ( metrics )","title":"generate_consumer_metrics()"},{"location":"monitoring/#consumer-metrics","text":"We advise including the consumer_lag in your application's grafana dashboard. consumer_lag will show you how far your consumers are lagging behind the published events in the topic they are reading. For instance, if you have a single consumer and another team is producing millions of events, the consumer might not be able to handle them in time (where in time is defined by you, like: \"in an hour of receiving a message it should be consumed\"). Based on the lag, you will have to develop your own alerts. An alert should be pushed to Slack if you experience more than a particular amount of lag. You will require your consumer_group name in order to design a basic dashboard using the consumer_lag . We could add a query in Grafana like this: sum ( kafka_consumer_group_ConsumerLagMetrics_Value { topic =~ \" YOUR_OWN_TOPIC_NAME \", groupId =~ \" YOUR_CONSUMER_GROUP \", name = \" SumOffsetLag \"} ) by ( topic ) Remember to replace YOUR_CONSUMER_GROUP and YOUR_OWN_TOPIC_NAME with your consumer_group and topic respectively \u2b06\ufe0f","title":"Consumer Metrics"},{"location":"monitoring/#producer-metrics","text":"If you have producers, it's a good idea to monitor the growth of Log End Offset (LEO). The increase in LEO indicates the number of events produced in the last N minutes. If you know that events should occur every N minutes, you can trigger alerts if no events occur because this metric will tell you whether or not events occurred. We could add a query in Grafana like this, where N is 10m : sum ( max ( increase ( kafka_log_Log_Value { name = \" LogEndOffset \", topic =~ \" TOPIC_NAME \"}[ 10m ] )) by ( partition , topic )) by ( topic ) Remember to modify TOPIC_NAME to the name of the topic you want to track \u2b06\ufe0f","title":"Producer Metrics"},{"location":"monitoring/#custom-business-metrics","text":"One benefit of Prometheus is that you can design your own custom metrics. Scenario : Consider an event-based ordering system. Assume you receive X orders daily and ship Y orders daily. Most likely, you will create a dashboard using this data. Fortunately, we can create our own custom metrics by using the Prometheus Python client. You can construct a variety of metrics with prometheus: Gauge Counter Histogram Summary You can read more about it in prometheus metric_types website. In our scenario, we will most likely want a Counter for orders received and a Counter for orders shipped. from prometheus_client import Counter from kstreams import PrometheusMonitor class MyAppPrometheusMonitor ( PrometheusMonitor ): def __init__ ( self ): super () . __init__ () # initialize kstream metrics self . orders_received = Counter ( 'orders_received' , 'Amount of orders received' ) self . orders_shipped = Counter ( 'orders_shipped' , 'Amount of orders shipped' ) def increase_received ( self , amount : int = 1 ): self . orders_received . inc ( amount ) def increase_shipped ( self , amount : int = 1 ): self . orders_shipped . inc ( amount ) In our kstreams app, we can: stream_engine = create_engine ( title = \"my-engine\" , monitor = MyAppPrometheusMonitor ()) @stream_engine . stream ( \"my-special-orders\" ) async def consume_orders_received ( cr : ConsumerRecord ): if cr . value . status == \"NEW\" : stream_engine . monitor . increase_received () elif cr . value . status == \"SHIPPED\" : stream_engine . monitor . increase_shipped () Your app's prometheus would display this data, which you might utilize to build a stylish \u2728dashboard\u2728 interface. For further details, see the Prometheus python client documentation.","title":"Custom Business Metrics"},{"location":"serialization/","text":"Kafka's job is to move bytes from producer to consumers, through a topic. By default, this is what kstream does. import logging from kstreams import ConsumerRecord , Send , stream logger = logging . getLogger ( __name__ ) @stream ( \"local--hello-world\" , group_id = \"example-group\" ) async def consume ( cr : ConsumerRecord , send : Send ) -> None : logger . info ( f \"showing bytes: { cr . value } \" ) value = f \"Event confirmed. { cr . value } \" await send ( \"local--kstreams\" , value = value . encode (), key = \"1\" , ) As you can see the ConsumerRecord's value is bytes. In order to keep your code pythonic, we provide a mechanism to serialize/deserialize these bytes, into something more useful. This way, you can work with other data structures, like a dict or dataclasses . Sometimes it is easier to work with a dict in your app, give it to kstreams , and let it transform it into bytes to be delivered to Kafka. For this situation, you need to implement kstreams.serializers.Serializer . kstreams.serializers.Serializer Protocol used by the Stream to serialize. A Protocol is similar to other languages features like an interface or a trait. End users should provide their own class implementing this protocol. For example a JsonSerializer from typing import Optional , Dict import json class JsonSerializer : async def serialize ( self , payload : dict , headers : Optional [ Dict [ str , str ]] = None , serializer_kwargs : Optional [ Dict ] = None , ) -> bytes : \"\"\"Return UTF-8 encoded payload\"\"\" value = json . dumps ( payload ) return value . encode () Notice that you don't need to inherit anything, you just have to comply with the Protocol. Source code in kstreams/serializers.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 class Serializer ( Protocol ): \"\"\"Protocol used by the Stream to serialize. A Protocol is similar to other languages features like an interface or a trait. End users should provide their own class implementing this protocol. For example a `JsonSerializer` ```python from typing import Optional, Dict import json class JsonSerializer: async def serialize( self, payload: dict, headers: Optional[Dict[str, str]] = None, serializer_kwargs: Optional[Dict] = None, ) -> bytes: \\\"\"\" Return UTF - 8 encoded payload \\ \"\"\" value = json.dumps(payload) return value.encode() ``` Notice that you don't need to inherit anything, you just have to comply with the Protocol. \"\"\" async def serialize ( self , payload : Any , headers : Optional [ Headers ] = None , serializer_kwargs : Optional [ Dict ] = None , ) -> bytes : \"\"\" Implement this method to deserialize the data received from the topic. \"\"\" ... serialize ( payload , headers = None , serializer_kwargs = None ) async Implement this method to deserialize the data received from the topic. Source code in kstreams/serializers.py 71 72 73 74 75 76 77 78 79 80 async def serialize ( self , payload : Any , headers : Optional [ Headers ] = None , serializer_kwargs : Optional [ Dict ] = None , ) -> bytes : \"\"\" Implement this method to deserialize the data received from the topic. \"\"\" ... The other situation is when you consume from Kafka (or other brokers). Instead of dealing with bytes , you may want to receive in your function the dict ready to be used. For those cases, we need to use middleware . For example, we can implement a JsonMiddleware : from kstreams import middleware , ConsumerRecord class JsonDeserializerMiddleware ( middleware . BaseMiddleware ): async def __call__ ( self , cr : ConsumerRecord ): if cr . value is not None : data = json . loads ( cr . value . decode ()) cr . value = data return await self . next_call ( cr ) It is also possble to use kstreams.serializers.Deserializer for deserialization, but this will be deprecated kstreams.serializers.Deserializer Protocol used by the Stream to deserialize. A Protocol is similar to other languages features like an interface or a trait. End users should provide their own class implementing this protocol. For example a JsonDeserializer import json from kstreams import ConsumerRecord class JsonDeserializer : async def deserialize ( self , consumer_record : ConsumerRecord , ** kwargs ) -> ConsumerRecord : data = json . loads ( consumer_record . value . decode ()) consumer_record . value = data return consumer_record Source code in kstreams/serializers.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class Deserializer ( Protocol ): \"\"\"Protocol used by the Stream to deserialize. A Protocol is similar to other languages features like an interface or a trait. End users should provide their own class implementing this protocol. For example a `JsonDeserializer` ```python import json from kstreams import ConsumerRecord class JsonDeserializer: async def deserialize( self, consumer_record: ConsumerRecord, **kwargs ) -> ConsumerRecord: data = json.loads(consumer_record.value.decode()) consumer_record.value = data return consumer_record ``` \"\"\" async def deserialize ( self , consumer_record : ConsumerRecord , ** kwargs ) -> ConsumerRecord : \"\"\" Implement this method to deserialize the data received from the topic. \"\"\" ... deserialize ( consumer_record , ** kwargs ) async Implement this method to deserialize the data received from the topic. Source code in kstreams/serializers.py 32 33 34 35 36 37 38 async def deserialize ( self , consumer_record : ConsumerRecord , ** kwargs ) -> ConsumerRecord : \"\"\" Implement this method to deserialize the data received from the topic. \"\"\" ... Warning kstreams.serializers.Deserializer will be deprecated, use middlewares instead Usage Once you have written your serializer or deserializer, there are 2 ways of using them, in a generic fashion or per stream. Initialize the engine with your serializers By doing this all the streams will use these serializers by default. stream_engine = create_engine ( title = \"my-stream-engine\" , serializer = JsonSerializer (), ) Initilize streams with a deserializer and produce events with serializers from kstreams import middleware , ConsumerRecord @stream_engine . stream ( topic , middlewares = [ middleware . Middleware ( JsonDeserializerMiddleware )]) async def hello_stream ( cr : ConsumerRecord ): # remember event.value is now a dict print ( cr . value [ \"message\" ]) save_to_db ( cr ) await stream_engine . send ( topic , value = { \"message\" : \"test\" } headers = { \"content-type\" : consts . APPLICATION_JSON ,} key = \"1\" , )","title":"Serialization"},{"location":"serialization/#kstreams.serializers.Serializer","text":"Protocol used by the Stream to serialize. A Protocol is similar to other languages features like an interface or a trait. End users should provide their own class implementing this protocol. For example a JsonSerializer from typing import Optional , Dict import json class JsonSerializer : async def serialize ( self , payload : dict , headers : Optional [ Dict [ str , str ]] = None , serializer_kwargs : Optional [ Dict ] = None , ) -> bytes : \"\"\"Return UTF-8 encoded payload\"\"\" value = json . dumps ( payload ) return value . encode () Notice that you don't need to inherit anything, you just have to comply with the Protocol. Source code in kstreams/serializers.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 class Serializer ( Protocol ): \"\"\"Protocol used by the Stream to serialize. A Protocol is similar to other languages features like an interface or a trait. End users should provide their own class implementing this protocol. For example a `JsonSerializer` ```python from typing import Optional, Dict import json class JsonSerializer: async def serialize( self, payload: dict, headers: Optional[Dict[str, str]] = None, serializer_kwargs: Optional[Dict] = None, ) -> bytes: \\\"\"\" Return UTF - 8 encoded payload \\ \"\"\" value = json.dumps(payload) return value.encode() ``` Notice that you don't need to inherit anything, you just have to comply with the Protocol. \"\"\" async def serialize ( self , payload : Any , headers : Optional [ Headers ] = None , serializer_kwargs : Optional [ Dict ] = None , ) -> bytes : \"\"\" Implement this method to deserialize the data received from the topic. \"\"\" ...","title":"Serializer"},{"location":"serialization/#kstreams.serializers.Serializer.serialize","text":"Implement this method to deserialize the data received from the topic. Source code in kstreams/serializers.py 71 72 73 74 75 76 77 78 79 80 async def serialize ( self , payload : Any , headers : Optional [ Headers ] = None , serializer_kwargs : Optional [ Dict ] = None , ) -> bytes : \"\"\" Implement this method to deserialize the data received from the topic. \"\"\" ... The other situation is when you consume from Kafka (or other brokers). Instead of dealing with bytes , you may want to receive in your function the dict ready to be used. For those cases, we need to use middleware . For example, we can implement a JsonMiddleware : from kstreams import middleware , ConsumerRecord class JsonDeserializerMiddleware ( middleware . BaseMiddleware ): async def __call__ ( self , cr : ConsumerRecord ): if cr . value is not None : data = json . loads ( cr . value . decode ()) cr . value = data return await self . next_call ( cr ) It is also possble to use kstreams.serializers.Deserializer for deserialization, but this will be deprecated","title":"serialize()"},{"location":"serialization/#kstreams.serializers.Deserializer","text":"Protocol used by the Stream to deserialize. A Protocol is similar to other languages features like an interface or a trait. End users should provide their own class implementing this protocol. For example a JsonDeserializer import json from kstreams import ConsumerRecord class JsonDeserializer : async def deserialize ( self , consumer_record : ConsumerRecord , ** kwargs ) -> ConsumerRecord : data = json . loads ( consumer_record . value . decode ()) consumer_record . value = data return consumer_record Source code in kstreams/serializers.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class Deserializer ( Protocol ): \"\"\"Protocol used by the Stream to deserialize. A Protocol is similar to other languages features like an interface or a trait. End users should provide their own class implementing this protocol. For example a `JsonDeserializer` ```python import json from kstreams import ConsumerRecord class JsonDeserializer: async def deserialize( self, consumer_record: ConsumerRecord, **kwargs ) -> ConsumerRecord: data = json.loads(consumer_record.value.decode()) consumer_record.value = data return consumer_record ``` \"\"\" async def deserialize ( self , consumer_record : ConsumerRecord , ** kwargs ) -> ConsumerRecord : \"\"\" Implement this method to deserialize the data received from the topic. \"\"\" ...","title":"Deserializer"},{"location":"serialization/#kstreams.serializers.Deserializer.deserialize","text":"Implement this method to deserialize the data received from the topic. Source code in kstreams/serializers.py 32 33 34 35 36 37 38 async def deserialize ( self , consumer_record : ConsumerRecord , ** kwargs ) -> ConsumerRecord : \"\"\" Implement this method to deserialize the data received from the topic. \"\"\" ... Warning kstreams.serializers.Deserializer will be deprecated, use middlewares instead","title":"deserialize()"},{"location":"serialization/#usage","text":"Once you have written your serializer or deserializer, there are 2 ways of using them, in a generic fashion or per stream.","title":"Usage"},{"location":"serialization/#initialize-the-engine-with-your-serializers","text":"By doing this all the streams will use these serializers by default. stream_engine = create_engine ( title = \"my-stream-engine\" , serializer = JsonSerializer (), )","title":"Initialize the engine with your serializers"},{"location":"serialization/#initilize-streams-with-a-deserializer-and-produce-events-with-serializers","text":"from kstreams import middleware , ConsumerRecord @stream_engine . stream ( topic , middlewares = [ middleware . Middleware ( JsonDeserializerMiddleware )]) async def hello_stream ( cr : ConsumerRecord ): # remember event.value is now a dict print ( cr . value [ \"message\" ]) save_to_db ( cr ) await stream_engine . send ( topic , value = { \"message\" : \"test\" } headers = { \"content-type\" : consts . APPLICATION_JSON ,} key = \"1\" , )","title":"Initilize streams with a deserializer and produce events with serializers"},{"location":"stream/","text":"Streams A Stream in kstreams is an extension of AIOKafkaConsumer Consuming can be done using kstreams.Stream . You only need to decorate a coroutine with @stream_engine.streams . The decorator has the same aiokafka consumer API at initialization, in other words they accept the same args and kwargs that the aiokafka consumer accepts. kstreams.streams.Stream Attributes: Name Type Description name Optional [ str ] Stream name. Default is a generated uuid4 topics List [ str ] List of topics to consume subscribe_by_pattern bool Whether subscribe to topics by pattern backend Kafka backend kstreams.backends.kafka.Kafka: Backend to connect. Default Kafka func Callable [[ Stream ], Awaitable [ Any ]] Coroutine fucntion or generator to be called when an event arrives config Dict [ str , Any ] Stream configuration. Here all the properties can be passed in the dictionary deserializer Deserializer Deserializer to be used when an event is consumed initial_offsets List [ TopicPartitionOffset ] List of TopicPartitionOffset that will seek the initial offsets to rebalance_listener RebalanceListener Listener callbacks when partition are assigned or revoked Subscribe to a topic Example import aiorun from kstreams import create_engine , ConsumerRecord stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ( \"local--kstreams\" , group_id = \"my-group-id\" ) async def stream ( cr : ConsumerRecord ) -> None : print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . value } \" ) async def start (): await stream_engine . start () async def shutdown ( loop ): await stream_engine . stop () if __name__ == \"__main__\" : aiorun . run ( start (), stop_on_unhandled_errors = True , shutdown_callback = shutdown ) Subscribe to multiple topics Consuming from multiple topics using one stream is possible. A List[str] of topics must be provided. Example import aiorun from kstreams import create_engine , ConsumerRecord stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ( [ \"local--kstreams\" , \"local--hello-world\" ], group_id = \"my-group-id\" , ) async def consume ( cr : ConsumerRecord ) -> None : print ( f \"Event from { cr . topic } : headers: { cr . headers } , payload: { cr . value } \" ) Subscribe to topics by pattern In the following example the stream will subscribe to any topic that matches the regex ^dev--customer-.* , for example dev--customer-invoice or dev--customer-profile . The subscribe_by_pattern flag must be set to True . Example import aiorun from kstreams import create_engine , ConsumerRecord stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ( topics = \"^dev--customer-.*$\" , subscribe_by_pattern = True , group_id = \"my-group-id\" , ) async def stream ( cr : ConsumerRecord ) -> None : if cr . topic == \"dev--customer-invoice\" : print ( \"Event from topic dev--customer-invoice\" elif cr . topic == \"dev--customer-profile\" : print ( \"Event from topic dev--customer-profile\" else : raise ValueError ( f \"Invalid topic { cr . topic } \" ) async def start (): await stream_engine . start () async def shutdown ( loop ): await stream_engine . stop () if __name__ == \"__main__\" : aiorun . run ( start (), stop_on_unhandled_errors = True , shutdown_callback = shutdown ) Dependency Injection The old way to itereate over a stream is with the async for _ in stream loop. The iterable approach works but in most cases end users are interested only in the ConsumerRecord , for this reason it is possible to remove the async for loop using proper typing hints . The available typing hints are: ConsumerRecord : The aiokafka ConsumerRecord that will be received every time that a new event is in the Stream Stream : The Stream object that is subscribed to the topic/s. Useful when manual commit is enabled or when other Stream operations are needed Send : Coroutine to produce events. The same as stream_engine.send(...) if you use type hints then every time that a new event is in the stream the coroutine function defined by the end user will ba awaited with the specified types ConsumerRecord ConsumerRecord and Stream ConsumerRecord, Stream and Send Old fashion @stream_engine . stream ( topic ) async def my_stream ( cr : ConsumerRecord ): print ( cr . value ) @stream_engine . stream ( topic , enable_auto_commit = False ) async def my_stream ( cr : ConsumerRecord , stream : Stream ): print ( cr . value ) await stream . commit () @stream_engine . stream ( topic , enable_auto_commit = False ) async def my_stream ( cr : ConsumerRecord , stream : Stream , send : Send ): print ( cr . value ) await stream . commit () await send ( \"sink-to-elastic-topic\" , value = cr . value ) @stream_engine . stream ( topic ) async def consume ( stream ): # you can specify the type but it will be the same result async for cr in stream : print ( cr . value ) # you can do something with the stream as well!! Note The type arguments can be in any order. This might change in the future. Warning It is still possible to use the async for in loop, but it might be removed in the future. Migrate to the typing approach Creating a Stream instance If for any reason you need to create Streams instances directly, you can do it without using the decorator stream_engine.stream . Stream instance import aiorun from kstreams import create_engine , Stream , ConsumerRecord stream_engine = create_engine ( title = \"my-stream-engine\" ) class MyDeserializer : async def deserialize ( self , consumer_record : ConsumerRecord , ** kwargs ): return consumer_record . value . decode () async def stream ( cr : ConsumerRecord ) -> None : print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . value } \" ) stream = Stream ( \"local--kstreams\" , name = \"my-stream\" func = stream , # coroutine or async generator deserializer = MyDeserializer (), ) # add the stream to the engine stream_engine . add_stream ( stream ) async def start (): await stream_engine . start () await produce () async def shutdown ( loop ): await stream_engine . stop () if __name__ == \"__main__\" : aiorun . run ( start (), stop_on_unhandled_errors = True , shutdown_callback = shutdown ) Removing a stream from the engine Removing stream stream_engine . remove_stream ( stream ) Starting the stream with initial offsets If you want to start your consumption from certain offsets, you can include that in your stream instantiation. Use case: This feature is useful if one wants to manage their own offsets, rather than committing consumed offsets to Kafka. When an application manages its own offsets and tries to start a stream, we start the stream using the initial offsets as defined in the database. If you try to seek on a partition or topic that is not assigned to your stream, the code will ignore the seek and print out a warning. For example, if you have two consumers that are consuming from different partitions, and you try to seek for all of the partitions on each consumer, each consumer will seek for the partitions it has been assigned, and it will print out a warning log for the ones it was not assigned. If you try to seek on offsets that are not yet present on your partition, the consumer will revert to the auto_offset_reset config. There will not be a warning, so be aware of this. Also be aware that when your application restarts, it most likely will trigger the initial_offsets again. This means that setting intial_offsets to be a hardcoded number might not get the results you expect. Initial Offsets from Database from kstreams import Stream , structs topic_name = \"local--kstreams\" db_table = ExampleDatabase () initial_offset = structs . TopicPartitionOffset ( topic = topic_name , partition = 0 , offset = db_table . offset ) async def my_stream ( stream : Stream ): ... stream = Stream ( topic_name , name = \"my-stream\" , func = my_stream , # coroutine or async generator deserializer = MyDeserializer (), initial_offsets = [ initial_offset ], ) Stream crashing If your stream crashes for any reason the event consumption is stopped, meaning that non event will be consumed from the topic . However, it is possible to set three different error policies per stream: StreamErrorPolicy.STOP ( default ): Stop the Stream when an exception occurs. The exception is raised after the stream is properly stopped. StreamErrorPolicy.RESTART : Stop and restart the Stream when an exception occurs. The event that caused the exception is skipped. The exception is NOT raised because the application should contine working, however logger.exception() is used to alert the user. StreamErrorPolicy.STOP_ENGINE : Stop the StreamEngine when an exception occurs. The exception is raised after ALL the Streams were properly stopped. In the following example, the StreamErrorPolicy.RESTART error policy is specifed. If the Stream crashed with the ValueError exception it is restarted: from kstreams import create_engine , ConsumerRecord from kstreams.stream_utils import StreamErrorPolicy stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ( \"local--hello-world\" , group_id = \"example-group\" , error_policy = StreamErrorPolicy . RESTART ) async def stream ( cr : ConsumerRecord ) -> None : if cr . key == b \"error\" : # Stream will be restarted after the ValueError is raised raise ValueError ( \"error....\" ) print ( f \"Event consumed. Payload { cr . value } \" ) We can see the logs: ValueError: error.... INFO:aiokafka.consumer.group_coordinator:LeaveGroup request succeeded INFO:aiokafka.consumer.consumer:Unsubscribed all topics or patterns and assigned partitions INFO:kstreams.streams:Stream consuming from topics [ 'local--hello-world' ] has stopped!!! INFO:kstreams.middleware.middleware:Restarting stream <kstreams.streams.Stream object at 0x102d44050> INFO:aiokafka.consumer.subscription_state:Updating subscribed topics to: frozenset ({ 'local--hello-world' }) ... INFO:aiokafka.consumer.group_coordinator:Setting newly assigned partitions { TopicPartition ( topic = 'local--hello-world' , partition = 0 )} for group example-group Note If you are using aiorun with stop_on_unhandled_errors=True and the error_policy is StreamErrorPolicy.RESTART then the application will NOT stop as the exception that caused the Stream to crash is not raised Changing consumer behavior Most of the time you will only set the topic and the group_id to the consumer , but sometimes you might want more control over it, for example changing the policy for resetting offsets on OffsetOutOfRange errors or session timeout . To do this, you have to use the same kwargs as the aiokafka consumer API # The consumer sends periodic heartbeats every 500 ms # On OffsetOutOfRange errors, the offset will move to the oldest available message (\u2018earliest\u2019) @stream_engine . stream ( \"local--kstream\" , group_id = \"de-my-partition\" , session_timeout_ms = 500 , auto_offset_reset \"earliest\" ) async def stream ( cr : ConsumerRecord ): print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . value } \" ) Manual commit When processing more sensitive data and you want to be sure that the kafka offeset is commited once that you have done your tasks, you can use enable_auto_commit=False mode of Consumer. Manual commit example @stream_engine . stream ( \"local--kstream\" , group_id = \"de-my-partition\" , enable_auto_commit = False ) async def stream ( cr : ConsumerRecord , stream : Stream ): print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . value } \" ) # We need to make sure that the pyalod was stored before commiting the kafka offset await store_in_database ( payload ) await stream . commit () # You need to commit!!! Note This is a tradeoff from at most once to at least once delivery, to achieve exactly once you will need to save offsets in the destination database and validate those yourself. Yield from stream Sometimes is useful to yield values from a stream so you can consume events in your on phase or because you want to return results to the frontend (SSE example). If you use the yield keyword inside a coroutine it will be \"transform\" to a asynchronous generator function , meaning that inside there is an async generator and it can be consumed. Consuming an async generator is simple, you just use the async for in clause. Because consuming events only happens with the for loop , you have to make sure that the Stream has been started properly and after leaving the async for in the stream has been properly stopped. To facilitate the process, we have context manager that makes sure of the starting/stopping process. Yield example # Create your stream @stream_engine . stream ( \"local--kstream\" ) async def stream ( cr : ConsumerRecord , stream : Stream ): yield cr . value # Consume the stream: async with stream as stream_flow : # Use the context manager async for value in stream_flow : ... # do something with value (cr.value) Note If for some reason you interrupt the \"async for in\" in the async generator, the Stream will stopped consuming events meaning that the lag will increase. Note Yield from a stream only works with the typing approach Get many Get a batch of events from the assigned TopicPartition. Prefetched events are returned in batches by topic-partition. If messages is not available in the prefetched buffer this method waits timeout_ms milliseconds. Attributes: Name Type Description partitions List [ TopicPartition ] | None The partitions that need fetching message. If no one partition specified then all subscribed partitions will be used timeout_ms int | None milliseconds spent waiting if data is not available in the buffer. If 0, returns immediately with any records that are available currently in the buffer, else returns empty. Must not be negative. max_records int | None The amount of records to fetch. if timeout_ms was defined and reached and the fetched records has not reach max_records then returns immediately with any records that are available currently in the buffer Returns: Type Description Dict [ TopicPartition , List [ ConsumerRecord ]] Topic to list of records Example @stream_engine . stream ( topic , ... ) async def stream ( stream : Stream ): while True : data = await stream . getmany ( max_records = 5 ) print ( data ) Source code in kstreams/streams.py 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 async def getmany ( self , partitions : typing . Optional [ typing . List [ TopicPartition ]] = None , timeout_ms : int = 0 , max_records : typing . Optional [ int ] = None , ) -> typing . Dict [ TopicPartition , typing . List [ ConsumerRecord ]]: \"\"\" Get a batch of events from the assigned TopicPartition. Prefetched events are returned in batches by topic-partition. If messages is not available in the prefetched buffer this method waits `timeout_ms` milliseconds. Attributes: partitions List[TopicPartition] | None: The partitions that need fetching message. If no one partition specified then all subscribed partitions will be used timeout_ms int | None: milliseconds spent waiting if data is not available in the buffer. If 0, returns immediately with any records that are available currently in the buffer, else returns empty. Must not be negative. max_records int | None: The amount of records to fetch. if `timeout_ms` was defined and reached and the fetched records has not reach `max_records` then returns immediately with any records that are available currently in the buffer Returns: Topic to list of records !!! Example ```python @stream_engine.stream(topic, ...) async def stream(stream: Stream): while True: data = await stream.getmany(max_records=5) print(data) ``` \"\"\" partitions = partitions or [] return await self . consumer . getmany ( # type: ignore * partitions , timeout_ms = timeout_ms , max_records = max_records ) Warning This approach does not works with Dependency Injection . Rebalance Listener For some cases you will need a RebalanceListener so when partitions are assigned or revoked to the stream different accions can be performed. Use cases Cleanup or custom state save on the start of a rebalance operation Saving offsets in a custom store when a partition is revoked Load a state or cache warmup on completion of a successful partition re-assignment. Metrics Rebalance Listener Kstreams use a default listener for all the streams to clean the metrics after a rebalance takes place kstreams.MetricsRebalanceListener Source code in kstreams/rebalance_listener.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 class MetricsRebalanceListener ( RebalanceListener ): async def on_partitions_revoked ( self , revoked : typing . Set [ TopicPartition ]) -> None : \"\"\" Coroutine to be called *before* a rebalance operation starts and *after* the consumer stops fetching data. This will method will clean up the `Prometheus` metrics Attributes: revoked Set[TopicPartitions]: Partitions that were assigned to the consumer on the last rebalance \"\"\" # lock all asyncio Tasks so no new metrics will be added to the Monitor if revoked and self . engine is not None : async with asyncio . Lock (): if self . stream is not None and self . stream . consumer is not None : self . engine . monitor . clean_stream_consumer_metrics ( self . stream . consumer ) async def on_partitions_assigned ( self , assigned : typing . Set [ TopicPartition ] ) -> None : \"\"\" Coroutine to be called *after* partition re-assignment completes and *before* the consumer starts fetching data again. This method will start the `Prometheus` metrics Attributes: assigned Set[TopicPartition]: Partitions assigned to the consumer (may include partitions that were previously assigned) \"\"\" # lock all asyncio Tasks so no new metrics will be added to the Monitor if assigned and self . engine is not None : async with asyncio . Lock (): if self . stream is not None : self . stream . seek_to_initial_offsets () on_partitions_assigned ( assigned ) async Coroutine to be called after partition re-assignment completes and before the consumer starts fetching data again. This method will start the Prometheus metrics Attributes: Name Type Description assigned Set [ TopicPartition ] Partitions assigned to the consumer (may include partitions that were previously assigned) Source code in kstreams/rebalance_listener.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 async def on_partitions_assigned ( self , assigned : typing . Set [ TopicPartition ] ) -> None : \"\"\" Coroutine to be called *after* partition re-assignment completes and *before* the consumer starts fetching data again. This method will start the `Prometheus` metrics Attributes: assigned Set[TopicPartition]: Partitions assigned to the consumer (may include partitions that were previously assigned) \"\"\" # lock all asyncio Tasks so no new metrics will be added to the Monitor if assigned and self . engine is not None : async with asyncio . Lock (): if self . stream is not None : self . stream . seek_to_initial_offsets () on_partitions_revoked ( revoked ) async Coroutine to be called before a rebalance operation starts and after the consumer stops fetching data. This will method will clean up the Prometheus metrics Attributes: Name Type Description revoked Set [ TopicPartitions ] Partitions that were assigned to the consumer on the last rebalance Source code in kstreams/rebalance_listener.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 async def on_partitions_revoked ( self , revoked : typing . Set [ TopicPartition ]) -> None : \"\"\" Coroutine to be called *before* a rebalance operation starts and *after* the consumer stops fetching data. This will method will clean up the `Prometheus` metrics Attributes: revoked Set[TopicPartitions]: Partitions that were assigned to the consumer on the last rebalance \"\"\" # lock all asyncio Tasks so no new metrics will be added to the Monitor if revoked and self . engine is not None : async with asyncio . Lock (): if self . stream is not None and self . stream . consumer is not None : self . engine . monitor . clean_stream_consumer_metrics ( self . stream . consumer ) Manual Commit If manual commit is enabled, you migh want to use the ManualCommitRebalanceListener . This rebalance listener will call commit before the stream partitions are revoked to avoid the error CommitFailedError and duplicate message delivery after a rebalance. See code example with manual commit kstreams.ManualCommitRebalanceListener Source code in kstreams/rebalance_listener.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 class ManualCommitRebalanceListener ( MetricsRebalanceListener ): async def on_partitions_revoked ( self , revoked : typing . Set [ TopicPartition ]) -> None : \"\"\" Coroutine to be called *before* a rebalance operation starts and *after* the consumer stops fetching data. If manual commit is enabled, `commit` is called before the consumers partitions are revoked to prevent the error `CommitFailedError` and duplicate message delivery after a rebalance. Attributes: revoked Set[TopicPartitions]: Partitions that were assigned to the consumer on the last rebalance \"\"\" if ( revoked and self . stream is not None and self . stream . consumer is not None and not self . stream . consumer . _enable_auto_commit ): logger . info ( f \"Manual commit enabled for stream { self . stream } . \" \"Performing `commit` before revoking partitions\" ) async with asyncio . Lock (): await self . stream . commit () await super () . on_partitions_revoked ( revoked = revoked ) on_partitions_revoked ( revoked ) async Coroutine to be called before a rebalance operation starts and after the consumer stops fetching data. If manual commit is enabled, commit is called before the consumers partitions are revoked to prevent the error CommitFailedError and duplicate message delivery after a rebalance. Attributes: Name Type Description revoked Set [ TopicPartitions ] Partitions that were assigned to the consumer on the last rebalance Source code in kstreams/rebalance_listener.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 async def on_partitions_revoked ( self , revoked : typing . Set [ TopicPartition ]) -> None : \"\"\" Coroutine to be called *before* a rebalance operation starts and *after* the consumer stops fetching data. If manual commit is enabled, `commit` is called before the consumers partitions are revoked to prevent the error `CommitFailedError` and duplicate message delivery after a rebalance. Attributes: revoked Set[TopicPartitions]: Partitions that were assigned to the consumer on the last rebalance \"\"\" if ( revoked and self . stream is not None and self . stream . consumer is not None and not self . stream . consumer . _enable_auto_commit ): logger . info ( f \"Manual commit enabled for stream { self . stream } . \" \"Performing `commit` before revoking partitions\" ) async with asyncio . Lock (): await self . stream . commit () await super () . on_partitions_revoked ( revoked = revoked ) Note ManualCommitRebalanceListener also includes the MetricsRebalanceListener funcionality. Custom Rebalance Listener If you want to define a custom RebalanceListener , it has to inherits from kstreams.RebalanceListener . kstreams.RebalanceListener A callback interface that the user can implement to trigger custom actions when the set of partitions are assigned or revoked to the Stream . Example from kstreams import RebalanceListener , TopicPartition from .resource import stream_engine class MyRebalanceListener ( RebalanceListener ): async def on_partitions_revoked ( self , revoked : Set [ TopicPartition ] ) -> None : # Do something with the revoked partitions # or with the Stream print ( self . stream ) async def on_partitions_assigned ( self , assigned : Set [ TopicPartition ] ) -> None : # Do something with the assigned partitions # or with the Stream print ( self . stream ) @stream_engine . stream ( topic , rebalance_listener = MyRebalanceListener ()) async def my_stream ( stream : Stream ): async for event in stream : ... Source code in kstreams/rebalance_listener.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 class RebalanceListener ( ConsumerRebalanceListener ): \"\"\" A callback interface that the user can implement to trigger custom actions when the set of partitions are assigned or revoked to the `Stream`. !!! Example ```python from kstreams import RebalanceListener, TopicPartition from .resource import stream_engine class MyRebalanceListener(RebalanceListener): async def on_partitions_revoked( self, revoked: Set[TopicPartition] ) -> None: # Do something with the revoked partitions # or with the Stream print(self.stream) async def on_partitions_assigned( self, assigned: Set[TopicPartition] ) -> None: # Do something with the assigned partitions # or with the Stream print(self.stream) @stream_engine.stream(topic, rebalance_listener=MyRebalanceListener()) async def my_stream(stream: Stream): async for event in stream: ... ``` \"\"\" def __init__ ( self ) -> None : self . stream : typing . Optional [ \"Stream\" ] = None # engine added so it can react on rebalance events self . engine : typing . Optional [ \"StreamEngine\" ] = None async def on_partitions_revoked ( self , revoked : typing . Set [ TopicPartition ]) -> None : \"\"\" Coroutine to be called *before* a rebalance operation starts and *after* the consumer stops fetching data. If you are using manual commit you have to commit all consumed offsets here, to avoid duplicate message delivery after rebalance is finished. Use cases: - cleanup or custom state save on the start of a rebalance operation - saving offsets in a custom store Attributes: revoked Set[TopicPartitions]: Partitions that were assigned to the consumer on the last rebalance !!! note The `Stream` is available using `self.stream` \"\"\" ... # pragma: no cover async def on_partitions_assigned ( self , assigned : typing . Set [ TopicPartition ] ) -> None : \"\"\" Coroutine to be called *after* partition re-assignment completes and *before* the consumer starts fetching data again. It is guaranteed that all the processes in a consumer group will execute their `on_partitions_revoked` callback before any instance executes its `on_partitions_assigned` callback. Use cases: - Load a state or cache warmup on completion of a successful partition re-assignment. Attributes: assigned Set[TopicPartition]: Partitions assigned to the consumer (may include partitions that were previously assigned) !!! note The `Stream` is available using `self.stream` \"\"\" ... # pragma: no cover on_partitions_assigned ( assigned ) async Coroutine to be called after partition re-assignment completes and before the consumer starts fetching data again. It is guaranteed that all the processes in a consumer group will execute their on_partitions_revoked callback before any instance executes its on_partitions_assigned callback. Use cases Load a state or cache warmup on completion of a successful partition re-assignment. Attributes: Name Type Description assigned Set [ TopicPartition ] Partitions assigned to the consumer (may include partitions that were previously assigned) Note The Stream is available using self.stream Source code in kstreams/rebalance_listener.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 async def on_partitions_assigned ( self , assigned : typing . Set [ TopicPartition ] ) -> None : \"\"\" Coroutine to be called *after* partition re-assignment completes and *before* the consumer starts fetching data again. It is guaranteed that all the processes in a consumer group will execute their `on_partitions_revoked` callback before any instance executes its `on_partitions_assigned` callback. Use cases: - Load a state or cache warmup on completion of a successful partition re-assignment. Attributes: assigned Set[TopicPartition]: Partitions assigned to the consumer (may include partitions that were previously assigned) !!! note The `Stream` is available using `self.stream` \"\"\" ... # pragma: no cover on_partitions_revoked ( revoked ) async Coroutine to be called before a rebalance operation starts and after the consumer stops fetching data. If you are using manual commit you have to commit all consumed offsets here, to avoid duplicate message delivery after rebalance is finished. Use cases cleanup or custom state save on the start of a rebalance operation saving offsets in a custom store Attributes: Name Type Description revoked Set [ TopicPartitions ] Partitions that were assigned to the consumer on the last rebalance Note The Stream is available using self.stream Source code in kstreams/rebalance_listener.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 async def on_partitions_revoked ( self , revoked : typing . Set [ TopicPartition ]) -> None : \"\"\" Coroutine to be called *before* a rebalance operation starts and *after* the consumer stops fetching data. If you are using manual commit you have to commit all consumed offsets here, to avoid duplicate message delivery after rebalance is finished. Use cases: - cleanup or custom state save on the start of a rebalance operation - saving offsets in a custom store Attributes: revoked Set[TopicPartitions]: Partitions that were assigned to the consumer on the last rebalance !!! note The `Stream` is available using `self.stream` \"\"\" ... # pragma: no cover Note It also possible to inherits from ManualCommitRebalanceListener and MetricsRebalanceListener","title":"Stream"},{"location":"stream/#streams","text":"A Stream in kstreams is an extension of AIOKafkaConsumer Consuming can be done using kstreams.Stream . You only need to decorate a coroutine with @stream_engine.streams . The decorator has the same aiokafka consumer API at initialization, in other words they accept the same args and kwargs that the aiokafka consumer accepts.","title":"Streams"},{"location":"stream/#kstreams.streams.Stream","text":"Attributes: Name Type Description name Optional [ str ] Stream name. Default is a generated uuid4 topics List [ str ] List of topics to consume subscribe_by_pattern bool Whether subscribe to topics by pattern backend Kafka backend kstreams.backends.kafka.Kafka: Backend to connect. Default Kafka func Callable [[ Stream ], Awaitable [ Any ]] Coroutine fucntion or generator to be called when an event arrives config Dict [ str , Any ] Stream configuration. Here all the properties can be passed in the dictionary deserializer Deserializer Deserializer to be used when an event is consumed initial_offsets List [ TopicPartitionOffset ] List of TopicPartitionOffset that will seek the initial offsets to rebalance_listener RebalanceListener Listener callbacks when partition are assigned or revoked","title":"Stream"},{"location":"stream/#kstreams.streams.Stream--subscribe-to-a-topic","text":"Example import aiorun from kstreams import create_engine , ConsumerRecord stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ( \"local--kstreams\" , group_id = \"my-group-id\" ) async def stream ( cr : ConsumerRecord ) -> None : print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . value } \" ) async def start (): await stream_engine . start () async def shutdown ( loop ): await stream_engine . stop () if __name__ == \"__main__\" : aiorun . run ( start (), stop_on_unhandled_errors = True , shutdown_callback = shutdown )","title":"Subscribe to a topic"},{"location":"stream/#kstreams.streams.Stream--subscribe-to-multiple-topics","text":"Consuming from multiple topics using one stream is possible. A List[str] of topics must be provided. Example import aiorun from kstreams import create_engine , ConsumerRecord stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ( [ \"local--kstreams\" , \"local--hello-world\" ], group_id = \"my-group-id\" , ) async def consume ( cr : ConsumerRecord ) -> None : print ( f \"Event from { cr . topic } : headers: { cr . headers } , payload: { cr . value } \" )","title":"Subscribe to multiple topics"},{"location":"stream/#kstreams.streams.Stream--subscribe-to-topics-by-pattern","text":"In the following example the stream will subscribe to any topic that matches the regex ^dev--customer-.* , for example dev--customer-invoice or dev--customer-profile . The subscribe_by_pattern flag must be set to True . Example import aiorun from kstreams import create_engine , ConsumerRecord stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ( topics = \"^dev--customer-.*$\" , subscribe_by_pattern = True , group_id = \"my-group-id\" , ) async def stream ( cr : ConsumerRecord ) -> None : if cr . topic == \"dev--customer-invoice\" : print ( \"Event from topic dev--customer-invoice\" elif cr . topic == \"dev--customer-profile\" : print ( \"Event from topic dev--customer-profile\" else : raise ValueError ( f \"Invalid topic { cr . topic } \" ) async def start (): await stream_engine . start () async def shutdown ( loop ): await stream_engine . stop () if __name__ == \"__main__\" : aiorun . run ( start (), stop_on_unhandled_errors = True , shutdown_callback = shutdown )","title":"Subscribe to topics by pattern"},{"location":"stream/#dependency-injection","text":"The old way to itereate over a stream is with the async for _ in stream loop. The iterable approach works but in most cases end users are interested only in the ConsumerRecord , for this reason it is possible to remove the async for loop using proper typing hints . The available typing hints are: ConsumerRecord : The aiokafka ConsumerRecord that will be received every time that a new event is in the Stream Stream : The Stream object that is subscribed to the topic/s. Useful when manual commit is enabled or when other Stream operations are needed Send : Coroutine to produce events. The same as stream_engine.send(...) if you use type hints then every time that a new event is in the stream the coroutine function defined by the end user will ba awaited with the specified types ConsumerRecord ConsumerRecord and Stream ConsumerRecord, Stream and Send Old fashion @stream_engine . stream ( topic ) async def my_stream ( cr : ConsumerRecord ): print ( cr . value ) @stream_engine . stream ( topic , enable_auto_commit = False ) async def my_stream ( cr : ConsumerRecord , stream : Stream ): print ( cr . value ) await stream . commit () @stream_engine . stream ( topic , enable_auto_commit = False ) async def my_stream ( cr : ConsumerRecord , stream : Stream , send : Send ): print ( cr . value ) await stream . commit () await send ( \"sink-to-elastic-topic\" , value = cr . value ) @stream_engine . stream ( topic ) async def consume ( stream ): # you can specify the type but it will be the same result async for cr in stream : print ( cr . value ) # you can do something with the stream as well!! Note The type arguments can be in any order. This might change in the future. Warning It is still possible to use the async for in loop, but it might be removed in the future. Migrate to the typing approach","title":"Dependency Injection"},{"location":"stream/#creating-a-stream-instance","text":"If for any reason you need to create Streams instances directly, you can do it without using the decorator stream_engine.stream . Stream instance import aiorun from kstreams import create_engine , Stream , ConsumerRecord stream_engine = create_engine ( title = \"my-stream-engine\" ) class MyDeserializer : async def deserialize ( self , consumer_record : ConsumerRecord , ** kwargs ): return consumer_record . value . decode () async def stream ( cr : ConsumerRecord ) -> None : print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . value } \" ) stream = Stream ( \"local--kstreams\" , name = \"my-stream\" func = stream , # coroutine or async generator deserializer = MyDeserializer (), ) # add the stream to the engine stream_engine . add_stream ( stream ) async def start (): await stream_engine . start () await produce () async def shutdown ( loop ): await stream_engine . stop () if __name__ == \"__main__\" : aiorun . run ( start (), stop_on_unhandled_errors = True , shutdown_callback = shutdown )","title":"Creating a Stream instance"},{"location":"stream/#removing-a-stream-from-the-engine","text":"Removing stream stream_engine . remove_stream ( stream )","title":"Removing a stream from the engine"},{"location":"stream/#starting-the-stream-with-initial-offsets","text":"If you want to start your consumption from certain offsets, you can include that in your stream instantiation. Use case: This feature is useful if one wants to manage their own offsets, rather than committing consumed offsets to Kafka. When an application manages its own offsets and tries to start a stream, we start the stream using the initial offsets as defined in the database. If you try to seek on a partition or topic that is not assigned to your stream, the code will ignore the seek and print out a warning. For example, if you have two consumers that are consuming from different partitions, and you try to seek for all of the partitions on each consumer, each consumer will seek for the partitions it has been assigned, and it will print out a warning log for the ones it was not assigned. If you try to seek on offsets that are not yet present on your partition, the consumer will revert to the auto_offset_reset config. There will not be a warning, so be aware of this. Also be aware that when your application restarts, it most likely will trigger the initial_offsets again. This means that setting intial_offsets to be a hardcoded number might not get the results you expect. Initial Offsets from Database from kstreams import Stream , structs topic_name = \"local--kstreams\" db_table = ExampleDatabase () initial_offset = structs . TopicPartitionOffset ( topic = topic_name , partition = 0 , offset = db_table . offset ) async def my_stream ( stream : Stream ): ... stream = Stream ( topic_name , name = \"my-stream\" , func = my_stream , # coroutine or async generator deserializer = MyDeserializer (), initial_offsets = [ initial_offset ], )","title":"Starting the stream with initial offsets"},{"location":"stream/#stream-crashing","text":"If your stream crashes for any reason the event consumption is stopped, meaning that non event will be consumed from the topic . However, it is possible to set three different error policies per stream: StreamErrorPolicy.STOP ( default ): Stop the Stream when an exception occurs. The exception is raised after the stream is properly stopped. StreamErrorPolicy.RESTART : Stop and restart the Stream when an exception occurs. The event that caused the exception is skipped. The exception is NOT raised because the application should contine working, however logger.exception() is used to alert the user. StreamErrorPolicy.STOP_ENGINE : Stop the StreamEngine when an exception occurs. The exception is raised after ALL the Streams were properly stopped. In the following example, the StreamErrorPolicy.RESTART error policy is specifed. If the Stream crashed with the ValueError exception it is restarted: from kstreams import create_engine , ConsumerRecord from kstreams.stream_utils import StreamErrorPolicy stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ( \"local--hello-world\" , group_id = \"example-group\" , error_policy = StreamErrorPolicy . RESTART ) async def stream ( cr : ConsumerRecord ) -> None : if cr . key == b \"error\" : # Stream will be restarted after the ValueError is raised raise ValueError ( \"error....\" ) print ( f \"Event consumed. Payload { cr . value } \" ) We can see the logs: ValueError: error.... INFO:aiokafka.consumer.group_coordinator:LeaveGroup request succeeded INFO:aiokafka.consumer.consumer:Unsubscribed all topics or patterns and assigned partitions INFO:kstreams.streams:Stream consuming from topics [ 'local--hello-world' ] has stopped!!! INFO:kstreams.middleware.middleware:Restarting stream <kstreams.streams.Stream object at 0x102d44050> INFO:aiokafka.consumer.subscription_state:Updating subscribed topics to: frozenset ({ 'local--hello-world' }) ... INFO:aiokafka.consumer.group_coordinator:Setting newly assigned partitions { TopicPartition ( topic = 'local--hello-world' , partition = 0 )} for group example-group Note If you are using aiorun with stop_on_unhandled_errors=True and the error_policy is StreamErrorPolicy.RESTART then the application will NOT stop as the exception that caused the Stream to crash is not raised","title":"Stream crashing"},{"location":"stream/#changing-consumer-behavior","text":"Most of the time you will only set the topic and the group_id to the consumer , but sometimes you might want more control over it, for example changing the policy for resetting offsets on OffsetOutOfRange errors or session timeout . To do this, you have to use the same kwargs as the aiokafka consumer API # The consumer sends periodic heartbeats every 500 ms # On OffsetOutOfRange errors, the offset will move to the oldest available message (\u2018earliest\u2019) @stream_engine . stream ( \"local--kstream\" , group_id = \"de-my-partition\" , session_timeout_ms = 500 , auto_offset_reset \"earliest\" ) async def stream ( cr : ConsumerRecord ): print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . value } \" )","title":"Changing consumer behavior"},{"location":"stream/#manual-commit","text":"When processing more sensitive data and you want to be sure that the kafka offeset is commited once that you have done your tasks, you can use enable_auto_commit=False mode of Consumer. Manual commit example @stream_engine . stream ( \"local--kstream\" , group_id = \"de-my-partition\" , enable_auto_commit = False ) async def stream ( cr : ConsumerRecord , stream : Stream ): print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . value } \" ) # We need to make sure that the pyalod was stored before commiting the kafka offset await store_in_database ( payload ) await stream . commit () # You need to commit!!! Note This is a tradeoff from at most once to at least once delivery, to achieve exactly once you will need to save offsets in the destination database and validate those yourself.","title":"Manual commit"},{"location":"stream/#yield-from-stream","text":"Sometimes is useful to yield values from a stream so you can consume events in your on phase or because you want to return results to the frontend (SSE example). If you use the yield keyword inside a coroutine it will be \"transform\" to a asynchronous generator function , meaning that inside there is an async generator and it can be consumed. Consuming an async generator is simple, you just use the async for in clause. Because consuming events only happens with the for loop , you have to make sure that the Stream has been started properly and after leaving the async for in the stream has been properly stopped. To facilitate the process, we have context manager that makes sure of the starting/stopping process. Yield example # Create your stream @stream_engine . stream ( \"local--kstream\" ) async def stream ( cr : ConsumerRecord , stream : Stream ): yield cr . value # Consume the stream: async with stream as stream_flow : # Use the context manager async for value in stream_flow : ... # do something with value (cr.value) Note If for some reason you interrupt the \"async for in\" in the async generator, the Stream will stopped consuming events meaning that the lag will increase. Note Yield from a stream only works with the typing approach","title":"Yield from stream"},{"location":"stream/#get-many","text":"Get a batch of events from the assigned TopicPartition. Prefetched events are returned in batches by topic-partition. If messages is not available in the prefetched buffer this method waits timeout_ms milliseconds. Attributes: Name Type Description partitions List [ TopicPartition ] | None The partitions that need fetching message. If no one partition specified then all subscribed partitions will be used timeout_ms int | None milliseconds spent waiting if data is not available in the buffer. If 0, returns immediately with any records that are available currently in the buffer, else returns empty. Must not be negative. max_records int | None The amount of records to fetch. if timeout_ms was defined and reached and the fetched records has not reach max_records then returns immediately with any records that are available currently in the buffer Returns: Type Description Dict [ TopicPartition , List [ ConsumerRecord ]] Topic to list of records Example @stream_engine . stream ( topic , ... ) async def stream ( stream : Stream ): while True : data = await stream . getmany ( max_records = 5 ) print ( data ) Source code in kstreams/streams.py 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 async def getmany ( self , partitions : typing . Optional [ typing . List [ TopicPartition ]] = None , timeout_ms : int = 0 , max_records : typing . Optional [ int ] = None , ) -> typing . Dict [ TopicPartition , typing . List [ ConsumerRecord ]]: \"\"\" Get a batch of events from the assigned TopicPartition. Prefetched events are returned in batches by topic-partition. If messages is not available in the prefetched buffer this method waits `timeout_ms` milliseconds. Attributes: partitions List[TopicPartition] | None: The partitions that need fetching message. If no one partition specified then all subscribed partitions will be used timeout_ms int | None: milliseconds spent waiting if data is not available in the buffer. If 0, returns immediately with any records that are available currently in the buffer, else returns empty. Must not be negative. max_records int | None: The amount of records to fetch. if `timeout_ms` was defined and reached and the fetched records has not reach `max_records` then returns immediately with any records that are available currently in the buffer Returns: Topic to list of records !!! Example ```python @stream_engine.stream(topic, ...) async def stream(stream: Stream): while True: data = await stream.getmany(max_records=5) print(data) ``` \"\"\" partitions = partitions or [] return await self . consumer . getmany ( # type: ignore * partitions , timeout_ms = timeout_ms , max_records = max_records ) Warning This approach does not works with Dependency Injection .","title":"Get many"},{"location":"stream/#rebalance-listener","text":"For some cases you will need a RebalanceListener so when partitions are assigned or revoked to the stream different accions can be performed.","title":"Rebalance Listener"},{"location":"stream/#use-cases","text":"Cleanup or custom state save on the start of a rebalance operation Saving offsets in a custom store when a partition is revoked Load a state or cache warmup on completion of a successful partition re-assignment.","title":"Use cases"},{"location":"stream/#metrics-rebalance-listener","text":"Kstreams use a default listener for all the streams to clean the metrics after a rebalance takes place","title":"Metrics Rebalance Listener"},{"location":"stream/#kstreams.MetricsRebalanceListener","text":"Source code in kstreams/rebalance_listener.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 class MetricsRebalanceListener ( RebalanceListener ): async def on_partitions_revoked ( self , revoked : typing . Set [ TopicPartition ]) -> None : \"\"\" Coroutine to be called *before* a rebalance operation starts and *after* the consumer stops fetching data. This will method will clean up the `Prometheus` metrics Attributes: revoked Set[TopicPartitions]: Partitions that were assigned to the consumer on the last rebalance \"\"\" # lock all asyncio Tasks so no new metrics will be added to the Monitor if revoked and self . engine is not None : async with asyncio . Lock (): if self . stream is not None and self . stream . consumer is not None : self . engine . monitor . clean_stream_consumer_metrics ( self . stream . consumer ) async def on_partitions_assigned ( self , assigned : typing . Set [ TopicPartition ] ) -> None : \"\"\" Coroutine to be called *after* partition re-assignment completes and *before* the consumer starts fetching data again. This method will start the `Prometheus` metrics Attributes: assigned Set[TopicPartition]: Partitions assigned to the consumer (may include partitions that were previously assigned) \"\"\" # lock all asyncio Tasks so no new metrics will be added to the Monitor if assigned and self . engine is not None : async with asyncio . Lock (): if self . stream is not None : self . stream . seek_to_initial_offsets ()","title":"MetricsRebalanceListener"},{"location":"stream/#kstreams.MetricsRebalanceListener.on_partitions_assigned","text":"Coroutine to be called after partition re-assignment completes and before the consumer starts fetching data again. This method will start the Prometheus metrics Attributes: Name Type Description assigned Set [ TopicPartition ] Partitions assigned to the consumer (may include partitions that were previously assigned) Source code in kstreams/rebalance_listener.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 async def on_partitions_assigned ( self , assigned : typing . Set [ TopicPartition ] ) -> None : \"\"\" Coroutine to be called *after* partition re-assignment completes and *before* the consumer starts fetching data again. This method will start the `Prometheus` metrics Attributes: assigned Set[TopicPartition]: Partitions assigned to the consumer (may include partitions that were previously assigned) \"\"\" # lock all asyncio Tasks so no new metrics will be added to the Monitor if assigned and self . engine is not None : async with asyncio . Lock (): if self . stream is not None : self . stream . seek_to_initial_offsets ()","title":"on_partitions_assigned()"},{"location":"stream/#kstreams.MetricsRebalanceListener.on_partitions_revoked","text":"Coroutine to be called before a rebalance operation starts and after the consumer stops fetching data. This will method will clean up the Prometheus metrics Attributes: Name Type Description revoked Set [ TopicPartitions ] Partitions that were assigned to the consumer on the last rebalance Source code in kstreams/rebalance_listener.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 async def on_partitions_revoked ( self , revoked : typing . Set [ TopicPartition ]) -> None : \"\"\" Coroutine to be called *before* a rebalance operation starts and *after* the consumer stops fetching data. This will method will clean up the `Prometheus` metrics Attributes: revoked Set[TopicPartitions]: Partitions that were assigned to the consumer on the last rebalance \"\"\" # lock all asyncio Tasks so no new metrics will be added to the Monitor if revoked and self . engine is not None : async with asyncio . Lock (): if self . stream is not None and self . stream . consumer is not None : self . engine . monitor . clean_stream_consumer_metrics ( self . stream . consumer )","title":"on_partitions_revoked()"},{"location":"stream/#manual-commit_1","text":"If manual commit is enabled, you migh want to use the ManualCommitRebalanceListener . This rebalance listener will call commit before the stream partitions are revoked to avoid the error CommitFailedError and duplicate message delivery after a rebalance. See code example with manual commit","title":"Manual Commit"},{"location":"stream/#kstreams.ManualCommitRebalanceListener","text":"Source code in kstreams/rebalance_listener.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 class ManualCommitRebalanceListener ( MetricsRebalanceListener ): async def on_partitions_revoked ( self , revoked : typing . Set [ TopicPartition ]) -> None : \"\"\" Coroutine to be called *before* a rebalance operation starts and *after* the consumer stops fetching data. If manual commit is enabled, `commit` is called before the consumers partitions are revoked to prevent the error `CommitFailedError` and duplicate message delivery after a rebalance. Attributes: revoked Set[TopicPartitions]: Partitions that were assigned to the consumer on the last rebalance \"\"\" if ( revoked and self . stream is not None and self . stream . consumer is not None and not self . stream . consumer . _enable_auto_commit ): logger . info ( f \"Manual commit enabled for stream { self . stream } . \" \"Performing `commit` before revoking partitions\" ) async with asyncio . Lock (): await self . stream . commit () await super () . on_partitions_revoked ( revoked = revoked )","title":"ManualCommitRebalanceListener"},{"location":"stream/#kstreams.ManualCommitRebalanceListener.on_partitions_revoked","text":"Coroutine to be called before a rebalance operation starts and after the consumer stops fetching data. If manual commit is enabled, commit is called before the consumers partitions are revoked to prevent the error CommitFailedError and duplicate message delivery after a rebalance. Attributes: Name Type Description revoked Set [ TopicPartitions ] Partitions that were assigned to the consumer on the last rebalance Source code in kstreams/rebalance_listener.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 async def on_partitions_revoked ( self , revoked : typing . Set [ TopicPartition ]) -> None : \"\"\" Coroutine to be called *before* a rebalance operation starts and *after* the consumer stops fetching data. If manual commit is enabled, `commit` is called before the consumers partitions are revoked to prevent the error `CommitFailedError` and duplicate message delivery after a rebalance. Attributes: revoked Set[TopicPartitions]: Partitions that were assigned to the consumer on the last rebalance \"\"\" if ( revoked and self . stream is not None and self . stream . consumer is not None and not self . stream . consumer . _enable_auto_commit ): logger . info ( f \"Manual commit enabled for stream { self . stream } . \" \"Performing `commit` before revoking partitions\" ) async with asyncio . Lock (): await self . stream . commit () await super () . on_partitions_revoked ( revoked = revoked ) Note ManualCommitRebalanceListener also includes the MetricsRebalanceListener funcionality.","title":"on_partitions_revoked()"},{"location":"stream/#custom-rebalance-listener","text":"If you want to define a custom RebalanceListener , it has to inherits from kstreams.RebalanceListener .","title":"Custom Rebalance Listener"},{"location":"stream/#kstreams.RebalanceListener","text":"A callback interface that the user can implement to trigger custom actions when the set of partitions are assigned or revoked to the Stream . Example from kstreams import RebalanceListener , TopicPartition from .resource import stream_engine class MyRebalanceListener ( RebalanceListener ): async def on_partitions_revoked ( self , revoked : Set [ TopicPartition ] ) -> None : # Do something with the revoked partitions # or with the Stream print ( self . stream ) async def on_partitions_assigned ( self , assigned : Set [ TopicPartition ] ) -> None : # Do something with the assigned partitions # or with the Stream print ( self . stream ) @stream_engine . stream ( topic , rebalance_listener = MyRebalanceListener ()) async def my_stream ( stream : Stream ): async for event in stream : ... Source code in kstreams/rebalance_listener.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 class RebalanceListener ( ConsumerRebalanceListener ): \"\"\" A callback interface that the user can implement to trigger custom actions when the set of partitions are assigned or revoked to the `Stream`. !!! Example ```python from kstreams import RebalanceListener, TopicPartition from .resource import stream_engine class MyRebalanceListener(RebalanceListener): async def on_partitions_revoked( self, revoked: Set[TopicPartition] ) -> None: # Do something with the revoked partitions # or with the Stream print(self.stream) async def on_partitions_assigned( self, assigned: Set[TopicPartition] ) -> None: # Do something with the assigned partitions # or with the Stream print(self.stream) @stream_engine.stream(topic, rebalance_listener=MyRebalanceListener()) async def my_stream(stream: Stream): async for event in stream: ... ``` \"\"\" def __init__ ( self ) -> None : self . stream : typing . Optional [ \"Stream\" ] = None # engine added so it can react on rebalance events self . engine : typing . Optional [ \"StreamEngine\" ] = None async def on_partitions_revoked ( self , revoked : typing . Set [ TopicPartition ]) -> None : \"\"\" Coroutine to be called *before* a rebalance operation starts and *after* the consumer stops fetching data. If you are using manual commit you have to commit all consumed offsets here, to avoid duplicate message delivery after rebalance is finished. Use cases: - cleanup or custom state save on the start of a rebalance operation - saving offsets in a custom store Attributes: revoked Set[TopicPartitions]: Partitions that were assigned to the consumer on the last rebalance !!! note The `Stream` is available using `self.stream` \"\"\" ... # pragma: no cover async def on_partitions_assigned ( self , assigned : typing . Set [ TopicPartition ] ) -> None : \"\"\" Coroutine to be called *after* partition re-assignment completes and *before* the consumer starts fetching data again. It is guaranteed that all the processes in a consumer group will execute their `on_partitions_revoked` callback before any instance executes its `on_partitions_assigned` callback. Use cases: - Load a state or cache warmup on completion of a successful partition re-assignment. Attributes: assigned Set[TopicPartition]: Partitions assigned to the consumer (may include partitions that were previously assigned) !!! note The `Stream` is available using `self.stream` \"\"\" ... # pragma: no cover","title":"RebalanceListener"},{"location":"stream/#kstreams.RebalanceListener.on_partitions_assigned","text":"Coroutine to be called after partition re-assignment completes and before the consumer starts fetching data again. It is guaranteed that all the processes in a consumer group will execute their on_partitions_revoked callback before any instance executes its on_partitions_assigned callback. Use cases Load a state or cache warmup on completion of a successful partition re-assignment. Attributes: Name Type Description assigned Set [ TopicPartition ] Partitions assigned to the consumer (may include partitions that were previously assigned) Note The Stream is available using self.stream Source code in kstreams/rebalance_listener.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 async def on_partitions_assigned ( self , assigned : typing . Set [ TopicPartition ] ) -> None : \"\"\" Coroutine to be called *after* partition re-assignment completes and *before* the consumer starts fetching data again. It is guaranteed that all the processes in a consumer group will execute their `on_partitions_revoked` callback before any instance executes its `on_partitions_assigned` callback. Use cases: - Load a state or cache warmup on completion of a successful partition re-assignment. Attributes: assigned Set[TopicPartition]: Partitions assigned to the consumer (may include partitions that were previously assigned) !!! note The `Stream` is available using `self.stream` \"\"\" ... # pragma: no cover","title":"on_partitions_assigned()"},{"location":"stream/#kstreams.RebalanceListener.on_partitions_revoked","text":"Coroutine to be called before a rebalance operation starts and after the consumer stops fetching data. If you are using manual commit you have to commit all consumed offsets here, to avoid duplicate message delivery after rebalance is finished. Use cases cleanup or custom state save on the start of a rebalance operation saving offsets in a custom store Attributes: Name Type Description revoked Set [ TopicPartitions ] Partitions that were assigned to the consumer on the last rebalance Note The Stream is available using self.stream Source code in kstreams/rebalance_listener.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 async def on_partitions_revoked ( self , revoked : typing . Set [ TopicPartition ]) -> None : \"\"\" Coroutine to be called *before* a rebalance operation starts and *after* the consumer stops fetching data. If you are using manual commit you have to commit all consumed offsets here, to avoid duplicate message delivery after rebalance is finished. Use cases: - cleanup or custom state save on the start of a rebalance operation - saving offsets in a custom store Attributes: revoked Set[TopicPartitions]: Partitions that were assigned to the consumer on the last rebalance !!! note The `Stream` is available using `self.stream` \"\"\" ... # pragma: no cover Note It also possible to inherits from ManualCommitRebalanceListener and MetricsRebalanceListener","title":"on_partitions_revoked()"},{"location":"test_client/","text":"Testing To test streams and producers or perform e2e tests you can make use of the test_utils.TestStreamClient . The TestStreamClient aims to emulate as much as possible the kafka behaviour using asyncio.Queue . This is excellent because you can test quite easily your code without spinning up kafka , but this comes with some limitations. It is not possible to know beforehand how many topics exist, how many partitions per topic exist, the replication factor, current offsets, etc. So, the test client will create topics , partitions , assigments , etc on runtime. Each Stream in your application will have assigned 3 partitions per topic by default (0, 1 and 2) during test environment With the test client you can: Send events so you won't need to mock the producer Call the consumer code, then the client will make sure that all the events are consumed before leaving the async context Using TestStreamClient Import TestStreamClient . Create a TestStreamClient by passing the engine instance to it. Create functions with a name that starts with test_ (this is standard pytest conventions). Use the TestStreamClient object the same way as you do with engine . Write simple assert statements with the standard Python expressions that you need to check (again, standard pytest ). Example Let's assume that you have the following code example. The goal is to store all the consumed events in an EventStore for future analysis. # example.py import aiorun import typing from dataclasses import dataclass , field from kstreams import ConsumerRecord , create_engine from kstreams.streams import Stream topic = \"local--kstreams\" stream_engine = create_engine ( title = \"my-stream-engine\" ) @dataclass class EventStore : \"\"\" Store events in memory \"\"\" events : typing . List [ ConsumerRecord ] = field ( default_factory = list ) def add ( self , event : ConsumerRecord ) -> None : self . events . append ( event ) @property def total ( self ): return len ( self . events ) event_store = EventStore () @stream_engine . stream ( topic , group_id = \"example-group\" ) async def consume ( cr : ConsumerRecord ): event_store . add ( cr ) async def produce (): payload = b '{\"message\": \"Hello world!\"}' for _ in range ( 5 ): await stream_engine . send ( topic , value = payload , key = \"1\" ) await asyncio . sleep ( 2 ) async def start (): await stream_engine . start () await produce () async def shutdown ( loop ): await stream_engine . stop () def main (): aiorun . run ( start (), stop_on_unhandled_errors = True , shutdown_callback = shutdown ) Then you could have a test_stream.py file to test the code, you need to instanciate the TestStreamClient with the engine : # test_stream.py import pytest from kstreams.test_utils import TestStreamClient from example import stream_engine , event_store client = TestStreamClient ( stream_engine ) @pytest . mark . asyncio async def test_add_event_on_consume (): \"\"\" Produce some events and check that the EventStore is updated. \"\"\" topic = \"local--kstreams\" # Use the same topic as the stream event = b '{\"message\": \"Hello world!\"}' async with client : metadata = await client . send ( topic , value = event , key = \"1\" ) # send the event with the test client current_offset = metadata . offset assert metadata . topic == topic # send another event and check that the offset was incremented metadata = await client . send ( topic , value = b '{\"message\": \"Hello world!\"}' , key = \"1\" ) assert metadata . offset == current_offset + 1 # check that the event_store has 2 events stored assert event_store . total == 2 Note Notice that the produce coroutine is not used to send events in the test case. The TestStreamClient.send coroutine is used instead. This allows to test streams without having producer code in your application Testing the Commit In some cases your stream will commit, in this situation checking the commited partitions can be useful. import pytest from kstreams.test_utils import TestStreamClient from kstreams import ConsumerRecord , Stream , TopicPartition from .example import produce , stream_engine topic_name = \"local--kstreams-marcos\" value = b '{\"message\": \"Hello world!\"}' name = \"my-stream\" key = \"1\" partition = 2 tp = TopicPartition ( topic = topic_name , partition = partition , ) total_events = 10 @stream_engine . stream ( topic_name , name = name ) async def my_stream ( cr : ConsumerRecord , stream : Stream ): # commit every time that an event arrives await stream . commit ({ tp : cr . offset }) # test the code client = TestStreamClient ( stream_engine ) @pytest . mark . asyncio async def test_consumer_commit ( stream_engine : StreamEngine ): async with client : for _ in range ( 0 , total_events ): await client . send ( topic_name , partition = partition , value = value , key = key ) # check that everything was commited stream = stream_engine . get_stream ( name ) assert ( await stream . committed ( tp )) == total_events E2E test In the previous code example the application produces to and consumes from the same topic, then TestStreamClient.send is not needed because the engine.send is producing. For those situation you can just use your producer code and check that certain code was called. # test_example.py import pytest from kstreams.test_utils import TestStreamClient from .example import produce , stream_engine client = TestStreamClient ( stream_engine ) @pytest . mark . asyncio async def test_e2e_example (): \"\"\" Test that events are produce by the engine and consumed by the streams \"\"\" with patch ( \"example.on_consume\" ) as on_consume , patch ( \"example.on_produce\" ) as on_produce : async with client : await produce () on_produce . call_count == 5 on_consume . call_count == 5 Producer only In some scenarios, your application will only produce events and other application/s will consume it, but you want to make sure that the event was procuced in a proper way and the topic contains that event . # producer_example.py from kstreams import create_engine import aiorun import asyncio stream_engine = create_engine ( title = \"my-stream-engine\" ) async def produce ( topic : str , value : bytes , key : str ): # This could be a complicated function or something like a FastAPI view await stream_engine . send ( topic , value = value , key = key ) async def start (): await stream_engine . start () await produce () async def shutdown ( loop ): await stream_engine . stop () def main (): aiorun . run ( start (), stop_on_unhandled_errors = True , shutdown_callback = shutdown ) Then you could have a test_producer_example.py file to test the code: # test_producer_example.py import pytest from kstreams.test_utils import TestStreamClient from producer_example import stream_engine , produce client = TestStreamClient ( stream_engine ) @pytest . mark . asyncio async def test_event_produced (): topic_name = \"local--kstreams\" value = b '{\"message\": \"Hello world!\"}' key = \"1\" async with client : await produce ( topic = topic_name , value = value , key = key ) # use the produce code to send events # check that the event was placed in a topic in a proper way consumer_record = await client . get_event ( topic_name = topic_name ) assert consumer_record . value == value assert consumer_record . key == key Note Even thought the previous example is using a simple produce function, it shows what to do when the procuder code is encapsulated in other functions, for example a FastAPI view. Then you don't want to use client.send directly, just called the function that contains stream_engine.send(...) Defining extra topics For some uses cases is required to produce an event to a topic ( target topic ) after it was consumed ( source topic ). We are in control of the source topic because it has a stream associated with it and we want to consume events from it, however we might not be in control of the target topic . How can we consume an event from the target topic which has not a stream associated and the topic will be created only when a send is reached? The answer is to pre define the extra topics before the test cycle has started. Let's take a look an example: Let's imagine that we have the following code: from kstreams import ConsumerRecord from .engine import stream_engine @stream_engine . stream ( \"source-topic\" , name = name ) async def consume ( cr : ConsumerRecord ) -> None : # do something, for example save to db await save_to_db ( cr ) # then produce the event to the `target topic` await stream_engine . send ( \"target-topic\" , value = cr . value , key = cr . key , headers = cr . headers ) Here we can test two things: Sending an event to the source-topic and check that the event has been consumed and saved to the DB Check that the event was send to the target-topic Testing point 1 is straightforward: import pytest from kstreams.test_utils import TestStreamClient from .engine import stream_engine client = TestStreamClient ( stream_engine ) value = b '{\"message\": \"Hello world!\"}' key = \"my-key\" async with client : # produce to the topic that has a stream await client . send ( \"source-topic\" , value = value , key = key ) # check that the event was saved to the DB assert await db . get ( ... ) However to test the point 2 we need more effort as the TestStreamClient is not aware of the target topic until it reaches the send inside the consume coroutine. If we try to get the target topic event inside the async with context we will have an error: async with client : # produce to the topic that has a stream await client . send ( \"source-topic\" , value = value , key = key ) ... # Let's check if it was received by the target topic event = await client . get_event ( topic_name = \"target-topic\" ) ValueError : You might be trying to get the topic target - topic outside the ` client async context ` or trying to get an event from an empty topic target - topic . Make sure that the code is inside the async contextand the topic has events . We can solve this with a delay ( await asyncio.sleep(...) ) inside the async with context to give time to the TestStreamClient to create the topic, however if the buisness logic inside the consume is slow we need to add more delay, then it will become a race condition . To proper solve it, we can specify to the TestStreamClient the extra topics that we need during the test cycle. import pytest from kstreams.test_utils import TestStreamClient from .engine import stream_engine # tell the client to create the extra topics client = TestStreamClient ( stream_engine , topics = [ \"target-topic\" ]) value = b '{\"message\": \"Hello world!\"}' key = \"my-key\" async with client : # produce to the topic that has a stream await client . send ( \"source-topic\" , value = value , key = key ) # check that the event was saved to the DB assert await db . get ( ... ) # Let's check if it was received by the target topic event = await client . get_event ( topic_name = \"target-topic\" ) assert event . value == value assert event . key == key Topics subscribed by pattern When a Stream is using pattern subscription it is not possible to know before hand how many topics the Stream will consume from. To solve this problem the topics must be pre defined using the extra topics features from the TestClient : In the following example we have a Stream that will consume from topics that match the regular expression ^dev--customer-.*$ , for example dev--customer-invoice and dev--customer-profile . # app.py from kstreams import ConsumerRecord stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ( topics = \"^dev--customer-.*$\" , subscribe_by_pattern = True ) async def stream ( cr : ConsumerRecord ): if cr . topic == customer_invoice_topic : assert cr . value == invoice_event elif cr . topic == customer_profile_topic : assert cr . value == profile_event else : raise ValueError ( f \"Invalid topic { cr . topic } \" ) Then to test our Stream , we need to pre define the topics: # test_stream.py import pytest from kstreams.test_utils import TestStreamClient from app import stream_engine @pytest . mark . asyncio async def test_consume_events_topics_by_pattern (): \"\"\" This test shows the possibility to subscribe to multiple topics using a pattern \"\"\" customer_invoice_topic = \"dev--customer-invoice\" customer_profile_topic = \"dev--customer-profile\" client = TestStreamClient ( stream_engine , topics = [ customer_invoice_topic , customer_profile_topic ] ) async with client : await client . send ( customer_invoice_topic , value = b \"invoice-1\" , key = \"1\" ) await client . send ( customer_profile_topic , value = b \"profile-1\" , key = \"1\" ) # give some time to consume all the events await asyncio . sleep ( 0.1 ) assert TopicManager . all_messages_consumed () Disabling monitoring during testing Monitoring streams and producers is vital for streaming application but it requires extra effort. Sometimes during testing, monitoring is not required as we only want to focus on testing the buisness logic. In order to disable monitoring during testing use: client = TestStreamClient ( stream_engine , monitoring_enabled = False )","title":"Testing"},{"location":"test_client/#testing","text":"To test streams and producers or perform e2e tests you can make use of the test_utils.TestStreamClient . The TestStreamClient aims to emulate as much as possible the kafka behaviour using asyncio.Queue . This is excellent because you can test quite easily your code without spinning up kafka , but this comes with some limitations. It is not possible to know beforehand how many topics exist, how many partitions per topic exist, the replication factor, current offsets, etc. So, the test client will create topics , partitions , assigments , etc on runtime. Each Stream in your application will have assigned 3 partitions per topic by default (0, 1 and 2) during test environment With the test client you can: Send events so you won't need to mock the producer Call the consumer code, then the client will make sure that all the events are consumed before leaving the async context","title":"Testing"},{"location":"test_client/#using-teststreamclient","text":"Import TestStreamClient . Create a TestStreamClient by passing the engine instance to it. Create functions with a name that starts with test_ (this is standard pytest conventions). Use the TestStreamClient object the same way as you do with engine . Write simple assert statements with the standard Python expressions that you need to check (again, standard pytest ).","title":"Using TestStreamClient"},{"location":"test_client/#example","text":"Let's assume that you have the following code example. The goal is to store all the consumed events in an EventStore for future analysis. # example.py import aiorun import typing from dataclasses import dataclass , field from kstreams import ConsumerRecord , create_engine from kstreams.streams import Stream topic = \"local--kstreams\" stream_engine = create_engine ( title = \"my-stream-engine\" ) @dataclass class EventStore : \"\"\" Store events in memory \"\"\" events : typing . List [ ConsumerRecord ] = field ( default_factory = list ) def add ( self , event : ConsumerRecord ) -> None : self . events . append ( event ) @property def total ( self ): return len ( self . events ) event_store = EventStore () @stream_engine . stream ( topic , group_id = \"example-group\" ) async def consume ( cr : ConsumerRecord ): event_store . add ( cr ) async def produce (): payload = b '{\"message\": \"Hello world!\"}' for _ in range ( 5 ): await stream_engine . send ( topic , value = payload , key = \"1\" ) await asyncio . sleep ( 2 ) async def start (): await stream_engine . start () await produce () async def shutdown ( loop ): await stream_engine . stop () def main (): aiorun . run ( start (), stop_on_unhandled_errors = True , shutdown_callback = shutdown ) Then you could have a test_stream.py file to test the code, you need to instanciate the TestStreamClient with the engine : # test_stream.py import pytest from kstreams.test_utils import TestStreamClient from example import stream_engine , event_store client = TestStreamClient ( stream_engine ) @pytest . mark . asyncio async def test_add_event_on_consume (): \"\"\" Produce some events and check that the EventStore is updated. \"\"\" topic = \"local--kstreams\" # Use the same topic as the stream event = b '{\"message\": \"Hello world!\"}' async with client : metadata = await client . send ( topic , value = event , key = \"1\" ) # send the event with the test client current_offset = metadata . offset assert metadata . topic == topic # send another event and check that the offset was incremented metadata = await client . send ( topic , value = b '{\"message\": \"Hello world!\"}' , key = \"1\" ) assert metadata . offset == current_offset + 1 # check that the event_store has 2 events stored assert event_store . total == 2 Note Notice that the produce coroutine is not used to send events in the test case. The TestStreamClient.send coroutine is used instead. This allows to test streams without having producer code in your application","title":"Example"},{"location":"test_client/#testing-the-commit","text":"In some cases your stream will commit, in this situation checking the commited partitions can be useful. import pytest from kstreams.test_utils import TestStreamClient from kstreams import ConsumerRecord , Stream , TopicPartition from .example import produce , stream_engine topic_name = \"local--kstreams-marcos\" value = b '{\"message\": \"Hello world!\"}' name = \"my-stream\" key = \"1\" partition = 2 tp = TopicPartition ( topic = topic_name , partition = partition , ) total_events = 10 @stream_engine . stream ( topic_name , name = name ) async def my_stream ( cr : ConsumerRecord , stream : Stream ): # commit every time that an event arrives await stream . commit ({ tp : cr . offset }) # test the code client = TestStreamClient ( stream_engine ) @pytest . mark . asyncio async def test_consumer_commit ( stream_engine : StreamEngine ): async with client : for _ in range ( 0 , total_events ): await client . send ( topic_name , partition = partition , value = value , key = key ) # check that everything was commited stream = stream_engine . get_stream ( name ) assert ( await stream . committed ( tp )) == total_events","title":"Testing the Commit"},{"location":"test_client/#e2e-test","text":"In the previous code example the application produces to and consumes from the same topic, then TestStreamClient.send is not needed because the engine.send is producing. For those situation you can just use your producer code and check that certain code was called. # test_example.py import pytest from kstreams.test_utils import TestStreamClient from .example import produce , stream_engine client = TestStreamClient ( stream_engine ) @pytest . mark . asyncio async def test_e2e_example (): \"\"\" Test that events are produce by the engine and consumed by the streams \"\"\" with patch ( \"example.on_consume\" ) as on_consume , patch ( \"example.on_produce\" ) as on_produce : async with client : await produce () on_produce . call_count == 5 on_consume . call_count == 5","title":"E2E test"},{"location":"test_client/#producer-only","text":"In some scenarios, your application will only produce events and other application/s will consume it, but you want to make sure that the event was procuced in a proper way and the topic contains that event . # producer_example.py from kstreams import create_engine import aiorun import asyncio stream_engine = create_engine ( title = \"my-stream-engine\" ) async def produce ( topic : str , value : bytes , key : str ): # This could be a complicated function or something like a FastAPI view await stream_engine . send ( topic , value = value , key = key ) async def start (): await stream_engine . start () await produce () async def shutdown ( loop ): await stream_engine . stop () def main (): aiorun . run ( start (), stop_on_unhandled_errors = True , shutdown_callback = shutdown ) Then you could have a test_producer_example.py file to test the code: # test_producer_example.py import pytest from kstreams.test_utils import TestStreamClient from producer_example import stream_engine , produce client = TestStreamClient ( stream_engine ) @pytest . mark . asyncio async def test_event_produced (): topic_name = \"local--kstreams\" value = b '{\"message\": \"Hello world!\"}' key = \"1\" async with client : await produce ( topic = topic_name , value = value , key = key ) # use the produce code to send events # check that the event was placed in a topic in a proper way consumer_record = await client . get_event ( topic_name = topic_name ) assert consumer_record . value == value assert consumer_record . key == key Note Even thought the previous example is using a simple produce function, it shows what to do when the procuder code is encapsulated in other functions, for example a FastAPI view. Then you don't want to use client.send directly, just called the function that contains stream_engine.send(...)","title":"Producer only"},{"location":"test_client/#defining-extra-topics","text":"For some uses cases is required to produce an event to a topic ( target topic ) after it was consumed ( source topic ). We are in control of the source topic because it has a stream associated with it and we want to consume events from it, however we might not be in control of the target topic . How can we consume an event from the target topic which has not a stream associated and the topic will be created only when a send is reached? The answer is to pre define the extra topics before the test cycle has started. Let's take a look an example: Let's imagine that we have the following code: from kstreams import ConsumerRecord from .engine import stream_engine @stream_engine . stream ( \"source-topic\" , name = name ) async def consume ( cr : ConsumerRecord ) -> None : # do something, for example save to db await save_to_db ( cr ) # then produce the event to the `target topic` await stream_engine . send ( \"target-topic\" , value = cr . value , key = cr . key , headers = cr . headers ) Here we can test two things: Sending an event to the source-topic and check that the event has been consumed and saved to the DB Check that the event was send to the target-topic Testing point 1 is straightforward: import pytest from kstreams.test_utils import TestStreamClient from .engine import stream_engine client = TestStreamClient ( stream_engine ) value = b '{\"message\": \"Hello world!\"}' key = \"my-key\" async with client : # produce to the topic that has a stream await client . send ( \"source-topic\" , value = value , key = key ) # check that the event was saved to the DB assert await db . get ( ... ) However to test the point 2 we need more effort as the TestStreamClient is not aware of the target topic until it reaches the send inside the consume coroutine. If we try to get the target topic event inside the async with context we will have an error: async with client : # produce to the topic that has a stream await client . send ( \"source-topic\" , value = value , key = key ) ... # Let's check if it was received by the target topic event = await client . get_event ( topic_name = \"target-topic\" ) ValueError : You might be trying to get the topic target - topic outside the ` client async context ` or trying to get an event from an empty topic target - topic . Make sure that the code is inside the async contextand the topic has events . We can solve this with a delay ( await asyncio.sleep(...) ) inside the async with context to give time to the TestStreamClient to create the topic, however if the buisness logic inside the consume is slow we need to add more delay, then it will become a race condition . To proper solve it, we can specify to the TestStreamClient the extra topics that we need during the test cycle. import pytest from kstreams.test_utils import TestStreamClient from .engine import stream_engine # tell the client to create the extra topics client = TestStreamClient ( stream_engine , topics = [ \"target-topic\" ]) value = b '{\"message\": \"Hello world!\"}' key = \"my-key\" async with client : # produce to the topic that has a stream await client . send ( \"source-topic\" , value = value , key = key ) # check that the event was saved to the DB assert await db . get ( ... ) # Let's check if it was received by the target topic event = await client . get_event ( topic_name = \"target-topic\" ) assert event . value == value assert event . key == key","title":"Defining extra topics"},{"location":"test_client/#topics-subscribed-by-pattern","text":"When a Stream is using pattern subscription it is not possible to know before hand how many topics the Stream will consume from. To solve this problem the topics must be pre defined using the extra topics features from the TestClient : In the following example we have a Stream that will consume from topics that match the regular expression ^dev--customer-.*$ , for example dev--customer-invoice and dev--customer-profile . # app.py from kstreams import ConsumerRecord stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ( topics = \"^dev--customer-.*$\" , subscribe_by_pattern = True ) async def stream ( cr : ConsumerRecord ): if cr . topic == customer_invoice_topic : assert cr . value == invoice_event elif cr . topic == customer_profile_topic : assert cr . value == profile_event else : raise ValueError ( f \"Invalid topic { cr . topic } \" ) Then to test our Stream , we need to pre define the topics: # test_stream.py import pytest from kstreams.test_utils import TestStreamClient from app import stream_engine @pytest . mark . asyncio async def test_consume_events_topics_by_pattern (): \"\"\" This test shows the possibility to subscribe to multiple topics using a pattern \"\"\" customer_invoice_topic = \"dev--customer-invoice\" customer_profile_topic = \"dev--customer-profile\" client = TestStreamClient ( stream_engine , topics = [ customer_invoice_topic , customer_profile_topic ] ) async with client : await client . send ( customer_invoice_topic , value = b \"invoice-1\" , key = \"1\" ) await client . send ( customer_profile_topic , value = b \"profile-1\" , key = \"1\" ) # give some time to consume all the events await asyncio . sleep ( 0.1 ) assert TopicManager . all_messages_consumed ()","title":"Topics subscribed by pattern"},{"location":"test_client/#disabling-monitoring-during-testing","text":"Monitoring streams and producers is vital for streaming application but it requires extra effort. Sometimes during testing, monitoring is not required as we only want to focus on testing the buisness logic. In order to disable monitoring during testing use: client = TestStreamClient ( stream_engine , monitoring_enabled = False )","title":"Disabling monitoring during testing"},{"location":"utils/","text":"Utils Utility functions kstreams.utils create_ssl_context ( * , cafile = None , capath = None , cadata = None , certfile = None , keyfile = None , password = None , crlfile = None ) Wrapper of aiokafka.helpers.create_ssl_context with typehints. Parameters: Name Type Description Default cafile Optional [ str ] Certificate Authority file path containing certificates used to sign broker certificates None capath Optional [ str ] Same as cafile , but points to a directory containing several CA certificates None cadata Union [ str , bytes , None] Same as cafile , but instead contains already read data in either ASCII or bytes format None certfile Optional [ str ] optional filename of file in PEM format containing the client certificate, as well as any CA certificates needed to establish the certificate's authenticity None keyfile Optional [ str ] optional filename containing the client private key. None password Optional [ str ] optional password to be used when loading the certificate chain None Source code in kstreams/utils.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def create_ssl_context ( * , cafile : Optional [ str ] = None , capath : Optional [ str ] = None , cadata : Union [ str , bytes , None ] = None , certfile : Optional [ str ] = None , keyfile : Optional [ str ] = None , password : Optional [ str ] = None , crlfile : Any = None , ): \"\"\"Wrapper of [aiokafka.helpers.create_ssl_context]( https://aiokafka.readthedocs.io/en/stable/api.html#helpers ) with typehints. Arguments: cafile: Certificate Authority file path containing certificates used to sign broker certificates capath: Same as `cafile`, but points to a directory containing several CA certificates cadata: Same as `cafile`, but instead contains already read data in either ASCII or bytes format certfile: optional filename of file in PEM format containing the client certificate, as well as any CA certificates needed to establish the certificate's authenticity keyfile: optional filename containing the client private key. password: optional password to be used when loading the certificate chain \"\"\" return aiokafka_create_ssl_context ( cafile = cafile , capath = capath , cadata = cadata , certfile = certfile , keyfile = keyfile , password = password , crlfile = crlfile , ) create_ssl_context_from_mem ( * , certdata , keydata , password = None , cadata = None ) Create a SSL context from data on memory. This makes it easy to read the certificates from environmental variables Usually the data is loaded from env variables. Parameters: Name Type Description Default cadata Optional [ str ] certificates used to sign broker certificates provided as unicode str None certdata str the client certificate, as well as any CA certificates needed to establish the certificate's authenticity provided as unicode str required keydata str the client private key provided as unicode str required password Optional [ str ] optional password to be used when loading the certificate chain None Source code in kstreams/utils.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def create_ssl_context_from_mem ( * , certdata : str , keydata : str , password : Optional [ str ] = None , cadata : Optional [ str ] = None , ) -> Optional [ ssl . SSLContext ]: \"\"\"Create a SSL context from data on memory. This makes it easy to read the certificates from environmental variables Usually the data is loaded from env variables. Arguments: cadata: certificates used to sign broker certificates provided as unicode str certdata: the client certificate, as well as any CA certificates needed to establish the certificate's authenticity provided as unicode str keydata: the client private key provided as unicode str password: optional password to be used when loading the certificate chain \"\"\" with contextlib . ExitStack () as stack : cert_file = stack . enter_context ( NamedTemporaryFile ( suffix = \".crt\" )) key_file = stack . enter_context ( NamedTemporaryFile ( suffix = \".key\" )) # expecting unicode data, writing it as bytes to files as utf-8 cert_file . write ( certdata . encode ( \"utf-8\" )) cert_file . flush () key_file . write ( keydata . encode ( \"utf-8\" )) key_file . flush () ssl_context = ssl . create_default_context ( cadata = cadata ) ssl_context . load_cert_chain ( cert_file . name , keyfile = key_file . name , password = password ) return ssl_context return None","title":"Utils"},{"location":"utils/#utils","text":"Utility functions","title":"Utils"},{"location":"utils/#kstreams.utils","text":"","title":"utils"},{"location":"utils/#kstreams.utils.create_ssl_context","text":"Wrapper of aiokafka.helpers.create_ssl_context with typehints. Parameters: Name Type Description Default cafile Optional [ str ] Certificate Authority file path containing certificates used to sign broker certificates None capath Optional [ str ] Same as cafile , but points to a directory containing several CA certificates None cadata Union [ str , bytes , None] Same as cafile , but instead contains already read data in either ASCII or bytes format None certfile Optional [ str ] optional filename of file in PEM format containing the client certificate, as well as any CA certificates needed to establish the certificate's authenticity None keyfile Optional [ str ] optional filename containing the client private key. None password Optional [ str ] optional password to be used when loading the certificate chain None Source code in kstreams/utils.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def create_ssl_context ( * , cafile : Optional [ str ] = None , capath : Optional [ str ] = None , cadata : Union [ str , bytes , None ] = None , certfile : Optional [ str ] = None , keyfile : Optional [ str ] = None , password : Optional [ str ] = None , crlfile : Any = None , ): \"\"\"Wrapper of [aiokafka.helpers.create_ssl_context]( https://aiokafka.readthedocs.io/en/stable/api.html#helpers ) with typehints. Arguments: cafile: Certificate Authority file path containing certificates used to sign broker certificates capath: Same as `cafile`, but points to a directory containing several CA certificates cadata: Same as `cafile`, but instead contains already read data in either ASCII or bytes format certfile: optional filename of file in PEM format containing the client certificate, as well as any CA certificates needed to establish the certificate's authenticity keyfile: optional filename containing the client private key. password: optional password to be used when loading the certificate chain \"\"\" return aiokafka_create_ssl_context ( cafile = cafile , capath = capath , cadata = cadata , certfile = certfile , keyfile = keyfile , password = password , crlfile = crlfile , )","title":"create_ssl_context()"},{"location":"utils/#kstreams.utils.create_ssl_context_from_mem","text":"Create a SSL context from data on memory. This makes it easy to read the certificates from environmental variables Usually the data is loaded from env variables. Parameters: Name Type Description Default cadata Optional [ str ] certificates used to sign broker certificates provided as unicode str None certdata str the client certificate, as well as any CA certificates needed to establish the certificate's authenticity provided as unicode str required keydata str the client private key provided as unicode str required password Optional [ str ] optional password to be used when loading the certificate chain None Source code in kstreams/utils.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def create_ssl_context_from_mem ( * , certdata : str , keydata : str , password : Optional [ str ] = None , cadata : Optional [ str ] = None , ) -> Optional [ ssl . SSLContext ]: \"\"\"Create a SSL context from data on memory. This makes it easy to read the certificates from environmental variables Usually the data is loaded from env variables. Arguments: cadata: certificates used to sign broker certificates provided as unicode str certdata: the client certificate, as well as any CA certificates needed to establish the certificate's authenticity provided as unicode str keydata: the client private key provided as unicode str password: optional password to be used when loading the certificate chain \"\"\" with contextlib . ExitStack () as stack : cert_file = stack . enter_context ( NamedTemporaryFile ( suffix = \".crt\" )) key_file = stack . enter_context ( NamedTemporaryFile ( suffix = \".key\" )) # expecting unicode data, writing it as bytes to files as utf-8 cert_file . write ( certdata . encode ( \"utf-8\" )) cert_file . flush () key_file . write ( keydata . encode ( \"utf-8\" )) key_file . flush () ssl_context = ssl . create_default_context ( cadata = cadata ) ssl_context . load_cert_chain ( cert_file . name , keyfile = key_file . name , password = password ) return ssl_context return None","title":"create_ssl_context_from_mem()"}]}